import torch
import torch.optim as optim
from typing import Optional, Dict, List

# -------------------------- CSANetä¼˜åŒ–å™¨å…¨å±€é…ç½®ï¼ˆä¸¥æ ¼éµå¾ªè®ºæ–‡ï¼‰ --------------------------
OPTIM_CONFIG = {
    "train_stage": {
        "step_i_epoch": 20,    # Step-Iï¼ˆå•æ¨¡æ€èšç±»ï¼‰æ€»epochæ•°ï¼ˆğŸ”¶1-210ï¼‰
        "step_ii_epoch": 40,   # Step-IIï¼ˆè·¨æ¨¡æ€å…³è”ï¼‰æ€»epochæ•°
        "total_epoch": 60      # æ€»è®­ç»ƒepochæ•°ï¼ˆ20+40ï¼‰
    },
    "lr": {
        "base_lr": 3e-4,       # éª¨å¹²ç½‘ç»œåŸºç¡€å­¦ä¹ ç‡ï¼ˆTransformeréª¨å¹²ï¼ŒğŸ”¶1-208ï¼‰
        "module_lr_ratio": 10, # æ–°å¢æ¨¡å—ï¼ˆTBGM/CAP/IPCCï¼‰lrä¸ºéª¨å¹²çš„10å€ï¼ˆğŸ”¶1-209ï¼‰
        "warmup_epoch": 5,     # Step-Iå‰5 epochçº¿æ€§å‡æ¸©ï¼ˆé¿å…åˆå§‹lrè¿‡ä½ï¼‰
        "decay_steps": [40, 50], # Step-IIé˜¶æ¢¯è¡°å‡èŠ‚ç‚¹ï¼ˆ40/50 epochï¼Œå¯¹åº”æ€»epoch 40=20+20ã€50=20+30ï¼‰
        "decay_gamma": 0.1      # è¡°å‡ç³»æ•°ï¼ˆæ¯æ¬¡è¡°å‡ä¸ºä¹‹å‰çš„1/10ï¼ŒğŸ”¶1-209ï¼‰
    },
    "optim": {
        "type": "adam",        # è®ºæ–‡ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆå‚è€ƒTransformerå¸¸ç”¨é…ç½®ï¼‰
        "weight_decay": 5e-4,  # æƒé‡è¡°å‡ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼ŒğŸ”¶1-209ï¼‰
        "betas": (0.9, 0.999)  # Adamé»˜è®¤betaså‚æ•°
    }
}

#åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
def adjust_learning_rate(
    optimizer: optim.Optimizer,
    current_epoch: int,
    base_lr: float = OPTIM_CONFIG["lr"]["base_lr"],
    module_lr_ratio: int = OPTIM_CONFIG["lr"]["module_lr_ratio"]
) -> float:
    """
    æŒ‰CSANetåˆ†é˜¶æ®µè®­ç»ƒè°ƒæ•´å­¦ä¹ ç‡ï¼ˆğŸ”¶1-209ã€ğŸ”¶1-210 Algorithm 1ï¼‰
    Args:
        optimizer: å·²åˆå§‹åŒ–çš„ä¼˜åŒ–å™¨
        current_epoch: å½“å‰è®­ç»ƒepochï¼ˆ1-basedï¼ŒèŒƒå›´1~60ï¼‰
        base_lr: éª¨å¹²ç½‘ç»œåŸºç¡€å­¦ä¹ ç‡
        module_lr_ratio: æ–°å¢æ¨¡å—ä¸éª¨å¹²çš„lræ¯”ä¾‹
    Returns:
        current_base_lr: å½“å‰éª¨å¹²ç½‘ç»œå­¦ä¹ ç‡
    """
    # 1. åˆ¤æ–­å½“å‰è®­ç»ƒé˜¶æ®µï¼ˆStep-I: 1~20 epochï¼›Step-II: 21~60 epochï¼‰
    step_i_end = OPTIM_CONFIG["train_stage"]["step_i_epoch"]
    if not (1 <= current_epoch <= OPTIM_CONFIG["train_stage"]["total_epoch"]):
        raise ValueError(f"epochéœ€åœ¨1~{OPTIM_CONFIG['train_stage']['total_epoch']}ä¹‹é—´ï¼Œå½“å‰ä¸º{current_epoch}")
    
    # 2. è®¡ç®—å½“å‰åŸºç¡€å­¦ä¹ ç‡ï¼ˆéª¨å¹²ç½‘ç»œï¼‰
    if current_epoch <= step_i_end:
        # Step-Iï¼šå‰warmup_epochçº¿æ€§å‡æ¸©ï¼Œä¹‹åä¿æŒbase_lrï¼ˆğŸ”¶1-209ï¼‰
        warmup_epoch = OPTIM_CONFIG["lr"]["warmup_epoch"]
        if current_epoch <= warmup_epoch:
            # çº¿æ€§å‡æ¸©ï¼šlr = base_lr * (current_epoch / warmup_epoch)
            current_base_lr = base_lr * (current_epoch / warmup_epoch)
        else:
            # Step-IåæœŸï¼ˆ6~20 epochï¼‰ä¿æŒbase_lr
            current_base_lr = base_lr
    else:
        # Step-IIï¼šæŒ‰decay_stepsé˜¶æ¢¯è¡°å‡ï¼ˆğŸ”¶1-209ï¼‰
        current_base_lr = base_lr
        decay_steps = OPTIM_CONFIG["lr"]["decay_steps"]
        decay_gamma = OPTIM_CONFIG["lr"]["decay_gamma"]
        # è®¡ç®—è¡°å‡æ¬¡æ•°ï¼ˆä»…åœ¨Step-IIå†…åˆ¤æ–­ï¼‰
        decay_count = sum(1 for step in decay_steps if current_epoch > step)
        current_base_lr *= (decay_gamma ** decay_count)
    
    # 3. æ›´æ–°ä¼˜åŒ–å™¨å„å‚æ•°ç»„lrï¼ˆéª¨å¹²ç½‘ç»œç»„ *1ï¼Œæ–°å¢æ¨¡å—ç»„ *module_lr_ratioï¼‰
    # çº¦å®šï¼šå‚æ•°ç»„0ä¸ºéª¨å¹²ç½‘ç»œï¼Œ1~Nä¸ºæ–°å¢æ¨¡å—ï¼ˆTBGM/CAP/IPCCï¼‰
    if len(optimizer.param_groups) == 0:
        raise RuntimeError("ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–å‚æ•°ç»„")
    
    # æ›´æ–°éª¨å¹²ç½‘ç»œå‚æ•°ç»„ï¼ˆç»„0ï¼‰
    optimizer.param_groups[0]["lr"] = current_base_lr
    # æ›´æ–°æ–°å¢æ¨¡å—å‚æ•°ç»„ï¼ˆç»„1åŠä»¥åï¼‰
    module_lr = current_base_lr * module_lr_ratio
    for i in range(1, len(optimizer.param_groups)):
        optimizer.param_groups[i]["lr"] = module_lr
    
    # æ‰“å°å½“å‰lrä¿¡æ¯ï¼ˆé€‚é…è®ºæ–‡å®éªŒè®°å½•ï¼‰
    print(f"Epoch {current_epoch:2d} | é˜¶æ®µ: {'Step-I' if current_epoch <= step_i_end else 'Step-II'} | "
          f"éª¨å¹²lr: {current_base_lr:.6f} | æ¨¡å—lr: {module_lr:.6f}")
    
    return current_base_lr

def select_optimizer(
    args,
    main_net: torch.nn.Module,
    base_lr: float = OPTIM_CONFIG["lr"]["base_lr"],
    module_lr_ratio: int = OPTIM_CONFIG["lr"]["module_lr_ratio"]
) -> optim.Optimizer:
    """
    ä¸ºCSANeté€‰æ‹©ä¼˜åŒ–å™¨ï¼ŒæŒ‰æ¨¡å—åˆ†ç»„é…ç½®å·®å¼‚åŒ–å­¦ä¹ ç‡ï¼ˆğŸ”¶1-104 Fig.1ã€ğŸ”¶1-208ã€ğŸ”¶1-209ï¼‰
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ˆéœ€åŒ…å«args.optimï¼Œé»˜è®¤"adam"ï¼‰
        main_net: CSANetå®Œæ•´æ¨¡å‹ï¼ˆéœ€åŒ…å«æŒ‡å®šå­æ¨¡å—ï¼‰
        base_lr: éª¨å¹²ç½‘ç»œåŸºç¡€å­¦ä¹ ç‡
        module_lr_ratio: æ–°å¢æ¨¡å—ä¸éª¨å¹²çš„lræ¯”ä¾‹
    Returns:
        optimizer: é…ç½®å®Œæˆçš„Adamä¼˜åŒ–å™¨
    """
    # 1. æ ¡éªŒä¼˜åŒ–å™¨ç±»å‹ï¼ˆè®ºæ–‡ä»…ä½¿ç”¨Adamï¼‰
    if args.optim != OPTIM_CONFIG["optim"]["type"]:
        print(f"è­¦å‘Šï¼šè®ºæ–‡æ¨èä½¿ç”¨{OPTIM_CONFIG['optim']['type']}ä¼˜åŒ–å™¨ï¼Œå½“å‰ä¸º{args.optim}ï¼Œå°†è‡ªåŠ¨åˆ‡æ¢")
        args.optim = OPTIM_CONFIG["optim"]["type"]
    
    # 2. æŒ‰CSANetæ¨¡å—åˆ†ç»„å‚æ•°ï¼ˆæ’é™¤éå¯è®­ç»ƒå‚æ•°å¦‚è®°å¿†åº“ï¼‰
    # æ¨¡å—1ï¼šTransformeréª¨å¹²ç½‘ç»œï¼ˆç‰¹å¾æå–ï¼Œå¦‚main_net.backboneï¼‰
    # éœ€ç¡®ä¿æ¨¡å‹å®šä¹‰æ—¶éª¨å¹²ç½‘ç»œå‘½åä¸º"backbone"
    if not hasattr(main_net, "backbone"):
        raise AttributeError("CSANetæ¨¡å‹éœ€åŒ…å«'backbone'å­æ¨¡å—ï¼ˆTransformeréª¨å¹²ï¼Œå‚è€ƒTransReIDï¼‰")
    backbone_params = list(main_net.backbone.parameters())
    
    # æ¨¡å—2ï¼šTBGMæ¨¡å—ï¼ˆåŒäºŒåˆ†å›¾åŒ¹é…ï¼Œå¦‚main_net.tbgmï¼‰
    if not hasattr(main_net, "tbgm"):
        raise AttributeError("CSANetæ¨¡å‹éœ€åŒ…å«'tbgm'å­æ¨¡å—ï¼ˆğŸ”¶1-104 Fig.1ï¼‰")
    tbgm_params = list(main_net.tbgm.parameters())
    
    # æ¨¡å—3ï¼šCAP+IPCCæ¨¡å—ï¼ˆè·¨è¯¾ç¨‹å…³è”+ä¸€è‡´æ€§çº¦æŸï¼Œå¦‚main_net.capã€main_net.ipccï¼‰
    cap_ipcc_params = []
    if hasattr(main_net, "cap"):
        cap_ipcc_params.extend(list(main_net.cap.parameters()))
    if hasattr(main_net, "ipcc"):
        cap_ipcc_params.extend(list(main_net.ipcc.parameters()))
    if not cap_ipcc_params:
        raise AttributeError("CSANetæ¨¡å‹éœ€åŒ…å«'cap'å’Œ/æˆ–'ipcc'å­æ¨¡å—ï¼ˆğŸ”¶1-104 Fig.1ï¼‰")
    
    # 3. æ„å»ºå‚æ•°ç»„ï¼ˆéª¨å¹²ç»„lr=base_lrï¼Œå…¶ä»–ç»„lr=base_lr*module_lr_ratioï¼‰
    param_groups = [
        # ç»„0ï¼šéª¨å¹²ç½‘ç»œï¼ˆä½lrï¼Œé¿å…é¢„è®­ç»ƒå‚æ•°éœ‡è¡ï¼‰
        {"params": backbone_params, "lr": base_lr, "weight_decay": OPTIM_CONFIG["optim"]["weight_decay"]},
        # ç»„1ï¼šTBGMæ¨¡å—ï¼ˆé«˜lrï¼Œå¿«é€Ÿæ”¶æ•›ï¼‰
        {"params": tbgm_params, "lr": base_lr * module_lr_ratio, "weight_decay": OPTIM_CONFIG["optim"]["weight_decay"]},
        # ç»„2ï¼šCAP+IPCCæ¨¡å—ï¼ˆé«˜lrï¼Œå¿«é€Ÿæ”¶æ•›ï¼‰
        {"params": cap_ipcc_params, "lr": base_lr * module_lr_ratio, "weight_decay": OPTIM_CONFIG["optim"]["weight_decay"]}
    ]
    
    # 4. åˆå§‹åŒ–Adamä¼˜åŒ–å™¨ï¼ˆæŒ‰è®ºæ–‡é…ç½®ï¼‰
    optimizer = optim.Adam(
        param_groups,
        betas=OPTIM_CONFIG["optim"]["betas"],
        weight_decay=OPTIM_CONFIG["optim"]["weight_decay"]
    )
    
    # æ‰“å°å‚æ•°åˆ†ç»„ä¿¡æ¯ï¼ˆé€‚é…å®éªŒè®°å½•ï¼‰
    print(f"ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼ˆAdamï¼‰ï¼š")
    print(f"- éª¨å¹²ç½‘ç»œå‚æ•°æ•°ï¼š{sum(p.numel() for p in backbone_params) / 1e6:.2f}M | lr: {base_lr:.6f}")
    print(f"- TBGMæ¨¡å—å‚æ•°æ•°ï¼š{sum(p.numel() for p in tbgm_params) / 1e3:.2f}K | lr: {base_lr * module_lr_ratio:.6f}")
    print(f"- CAP+IPCCæ¨¡å—å‚æ•°æ•°ï¼š{sum(p.numel() for p in cap_ipcc_params) / 1e3:.2f}K | lr: {base_lr * module_lr_ratio:.6f}")
    
    return optimizer

def freeze_modules(
    main_net: torch.nn.Module,
    freeze_backbone: bool = False,
    freeze_tbgm_cap_ipcc: bool = True
) -> None:
    """
    å†»ç»“CSANetæŒ‡å®šæ¨¡å—ï¼ˆé€‚é…åˆ†é˜¶æ®µè®­ç»ƒï¼ŒğŸ”¶1-210 Algorithm 1ï¼‰
    Args:
        main_net: CSANetå®Œæ•´æ¨¡å‹
        freeze_backbone: æ˜¯å¦å†»ç»“éª¨å¹²ç½‘ç»œï¼ˆStep-IIå¯å†»ç»“é¢„è®­ç»ƒéª¨å¹²ï¼Œä»…å¾®è°ƒæ¨¡å—ï¼‰
        freeze_tbgm_cap_ipcc: æ˜¯å¦å†»ç»“TBGM/CAP/IPCCï¼ˆStep-Iéœ€å†»ç»“ï¼Œä»…è®­ç»ƒéª¨å¹²ï¼‰
    """
    # 1. å¤„ç†éª¨å¹²ç½‘ç»œå†»ç»“
    if hasattr(main_net, "backbone"):
        for param in main_net.backbone.parameters():
            param.requires_grad = not freeze_backbone
        print(f"éª¨å¹²ç½‘ç»œçŠ¶æ€ï¼š{'å†»ç»“' if freeze_backbone else 'å¯è®­ç»ƒ'}")
    
    # 2. å¤„ç†TBGM/CAP/IPCCå†»ç»“
    modules_to_freeze = []
    if hasattr(main_net, "tbgm"):
        modules_to_freeze.append(main_net.tbgm)
    if hasattr(main_net, "cap"):
        modules_to_freeze.append(main_net.cap)
    if hasattr(main_net, "ipcc"):
        modules_to_freeze.append(main_net.ipcc)
    
    for module in modules_to_freeze:
        for param in module.parameters():
            param.requires_grad = not freeze_tbgm_cap_ipcc
    print(f"TBGM/CAP/IPCCçŠ¶æ€ï¼š{'å†»ç»“' if freeze_tbgm_cap_ipcc else 'å¯è®­ç»ƒ'}")


def get_current_training_stage(current_epoch: int) -> str:
    """
    åˆ¤æ–­å½“å‰è®­ç»ƒé˜¶æ®µï¼ˆè¾…åŠ©æ—¥å¿—è®°å½•ï¼ŒğŸ”¶1-210ï¼‰
    """
    step_i_end = OPTIM_CONFIG["train_stage"]["step_i_epoch"]
    if current_epoch <= step_i_end:
        return "Step-I (Intra-modality Clustering)"
    else:
        return "Step-II (Cross-modality Curriculum Association)"