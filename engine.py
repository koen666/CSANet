import time
import numpy as np
import torch
from torch.autograd import Variable
from typing import Tuple, Dict, Optional, List
from tensorboardX import SummaryWriter  # æ–°å¢ï¼šå¯¼å…¥tensorboardXçš„SummaryWriter
from utils import AverageMeter, CSANetLogger, get_current_curriculum
from eval_metrics import eval_regdb, eval_sysu, format_metrics
from loss import CSANetTotalLoss
from otla_sk import calculate_pseudo_confidence

# -------------------------- CSANetè®­ç»ƒ/æµ‹è¯•å…¨å±€é…ç½®ï¼ˆä¸¥æ ¼éµå¾ªè®ºæ–‡ï¼‰ --------------------------
ENGINE_CONFIG = {
    "train_stage": {
        "step_i_epoch": 20,    # Step-Iæ€»epochæ•°ï¼ˆğŸ”¶1-210ï¼‰
        "step_ii_epoch": 40,   # Step-IIæ€»epochæ•°
        "total_epoch": 60      # æ€»epochæ•°
    },
    "loss_weights": {
        "step_i": {"lambda_nce": 1.0},  # Step-Iä»…å•æ¨¡æ€NCEæŸå¤±
        "step_ii": {"lambda_nce": 1.0, "lambda_cc": 1.0, "lambda_ipcc": 0.5}  # Step-IIä¸‰æŸå¤±æƒé‡ï¼ˆå…¬å¼21ï¼‰
    },
    "feature": {
        "feat_dim": 2048       # ç‰¹å¾ç»´åº¦ï¼ˆä¸Transformeréª¨å¹²è¾“å‡ºä¸€è‡´ï¼ŒğŸ”¶1-208ï¼‰
    },
    "test": {
        "sysu_modes": ["all", "indoor"],  # SYSU-MM01æµ‹è¯•æ¨¡å¼
        "regdb_modes": ["vis2thermal", "thermal2vis"]  # RegDBæµ‹è¯•æ¨¡å¼
    }
}

def get_training_stage(epoch: int) -> str:
    """åˆ¤æ–­å½“å‰è®­ç»ƒé˜¶æ®µï¼ˆStep-I/Step-IIï¼‰ï¼Œé€‚é…è®ºæ–‡Algorithm 1ï¼ˆğŸ”¶1-210ï¼‰"""
    if epoch <= ENGINE_CONFIG["train_stage"]["step_i_epoch"]:
        return "step_i"
    elif epoch <= ENGINE_CONFIG["train_stage"]["total_epoch"]:
        return "step_ii"
    else:
        raise ValueError(f"epochè¶…å‡ºèŒƒå›´ï¼ˆ1~{ENGINE_CONFIG['train_stage']['total_epoch']}ï¼‰ï¼Œå½“å‰ä¸º{epoch}")

def trainer(
    args,
    epoch: int,
    main_net: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    trainloader: torch.utils.data.DataLoader,
    total_loss_fn: CSANetTotalLoss,  # CSANetæ€»æŸå¤±è®¡ç®—å™¨
    logger: Optional[CSANetLogger] = None,
    writer: Optional[SummaryWriter] = None,
    # Step-IIä¸“å±è¾“å…¥
    curriculum_mask_vis: Optional[np.ndarray] = None,  # å¯è§å…‰è¯¾ç¨‹æ©ç 
    curriculum_mask_ir: Optional[np.ndarray] = None,   # çº¢å¤–è¯¾ç¨‹æ©ç 
    cap_mapping: Optional[Dict[str, Dict[int, int]]] = None,  # CAPå…³è”å­—å…¸ï¼ˆ{"vis2ir":..., "ir2vis":...}ï¼‰
    ref_memory: Optional[torch.Tensor] = None,  # IPCCå‚è€ƒè®°å¿†åº“ï¼ˆç®€å•è¯¾ç¨‹è®°å¿†åº“ï¼‰
    print_freq: int = 50
) -> Dict[str, float]:
    """
    CSANetè®­ç»ƒå™¨ï¼šåˆ†Step-I/IIè®­ç»ƒï¼Œæ”¯æŒè¯¾ç¨‹å­¦ä¹ ä¸æ ¸å¿ƒæ¨¡å—ååŒï¼ˆğŸ”¶1-210 Algorithm 1ï¼‰
    Args:
        cap_mapping: Step-IIä¸“å±ï¼ŒCAPä¼ é€’çš„è·¨æ¨¡æ€å…³è”å­—å…¸ï¼ˆvis2ir: vis_pidâ†’ir_pidï¼›ir2vis: ir_pidâ†’vis_pidï¼‰
        ref_memory: Step-IIä¸“å±ï¼ŒIPCCæ¨¡å—çš„å‚è€ƒè®°å¿†åº“ï¼ˆç®€å•è¯¾ç¨‹è®°å¿†åº“ï¼Œshape=[C_ref, feat_dim]ï¼‰
    Returns:
        train_stats: è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ï¼ˆå„æŸå¤±ã€å‡†ç¡®ç‡ï¼‰
    """
    # 1. åˆå§‹åŒ–é˜¶æ®µä¸å­¦ä¹ ç‡
    stage = get_training_stage(epoch)
    # è°ƒç”¨ä¿®æ”¹åçš„adjust_learning_rateï¼ˆoptimizer.pyä¸­å®šä¹‰ï¼‰è°ƒæ•´å­¦ä¹ ç‡
    current_lr = adjust_learning_rate(optimizer, current_epoch=epoch)

    # 2. åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
    metrics = {
        "total_loss": AverageMeter(),
        "nce_loss": AverageMeter(),       # å•æ¨¡æ€NCEæŸå¤±
        "cc_loss": AverageMeter(),        # è·¨æ¨¡æ€å¯¹æ¯”æŸå¤±ï¼ˆStep-IIï¼‰
        "ipcc_loss": AverageMeter(),      # IPCCä¸€è‡´æ€§æŸå¤±ï¼ˆStep-IIï¼‰
        "batch_time": AverageMeter(),
        "data_time": AverageMeter()
    }
    acc_metrics = {
        "nce_acc_vis": 0,  # å¯è§å…‰NCEå‡†ç¡®ç‡
        "nce_acc_ir": 0,   # çº¢å¤–NCEå‡†ç¡®ç‡
        "cc_acc_vis2ir": 0,  # å¯è§å…‰â†’çº¢å¤–CCå‡†ç¡®ç‡
        "cc_acc_ir2vis": 0,  # çº¢å¤–â†’å¯è§å…‰CCå‡†ç¡®ç‡
        "ipcc_prob_sim": 0   # IPCCå®ä¾‹-åŸå‹æ¦‚ç‡ç›¸ä¼¼åº¦
    }
    num_samples = {"vis": 0, "ir": 0}

    # 3. æ¨¡å‹åˆ‡æ¢è‡³è®­ç»ƒæ¨¡å¼
    main_net.train()
    end = time.time()

    # 4. è¿­ä»£è®­ç»ƒ
    for batch_id, (input_vis, input_ir, label_vis, label_ir) in enumerate(trainloader):
        # 4.1 æ•°æ®é¢„å¤„ç†ï¼ˆè®¾å¤‡å¯¹é½ã€è¯¾ç¨‹ç­›é€‰ï¼‰
        data_time = time.time() - end
        metrics["data_time"].update(data_time)

        # æ•°æ®è®¾å¤‡å¯¹é½
        input_vis = input_vis.cuda()
        input_ir = input_ir.cuda()
        label_vis = label_vis.cuda()
        label_ir = label_ir.cuda()
        B_vis, B_ir = input_vis.size(0), input_ir.size(0)
        num_samples["vis"] += B_vis
        num_samples["ir"] += B_ir

        # Step-IIè¯¾ç¨‹ç­›é€‰ï¼šä»…ä¿ç•™å½“å‰è¯¾ç¨‹çš„æ ·æœ¬ï¼ˆplain/moderate/intricateï¼‰
        if stage == "step_ii" and curriculum_mask_vis is not None and curriculum_mask_ir is not None:
            # è·å–å½“å‰è¯¾ç¨‹é˜¶æ®µï¼ˆplain/moderate/intricateï¼‰
            current_course = get_current_curriculum(epoch)
            course2level = {"plain": 0, "moderate": 1, "intricate": 2}
            target_level = course2level[current_course]

            # ç­›é€‰å¯è§å…‰å½“å‰è¯¾ç¨‹æ ·æœ¬
            vis_mask = (curriculum_mask_vis[num_samples["vis"]-B_vis : num_samples["vis"]] == target_level)
            input_vis = input_vis[vis_mask]
            label_vis = label_vis[vis_mask]
            B_vis = input_vis.size(0)

            # ç­›é€‰çº¢å¤–å½“å‰è¯¾ç¨‹æ ·æœ¬
            ir_mask = (curriculum_mask_ir[num_samples["ir"]-B_ir : num_samples["ir"]] == target_level)
            input_ir = input_ir[ir_mask]
            label_ir = label_ir[ir_mask]
            B_ir = input_ir.size(0)

            # è·³è¿‡ç©ºbatch
            if B_vis == 0 or B_ir == 0:
                end = time.time()
                continue

        # 4.2 æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆæå–ç‰¹å¾ä¸ä¼ªæ ‡ç­¾ï¼‰
        # é€‚é…CSANetæ¨¡å‹è¾“å‡ºï¼š(vis_feat, ir_feat, vis_pseudo_prob, ir_pseudo_prob)
        # modal=0è¡¨ç¤ºè®­ç»ƒæ¨¡å¼ï¼Œè¾“å‡ºåŒæ¨¡æ€ç‰¹å¾ä¸ä¼ªæ ‡ç­¾æ¦‚ç‡
        vis_feat, ir_feat, vis_pseudo_prob, ir_pseudo_prob = main_net(
            input_vis, input_ir, modal=0, train_set=True
        )

        # 4.3 å‡†å¤‡æŸå¤±è®¡ç®—è¾“å…¥ï¼ˆæŒ‰é˜¶æ®µåŒºåˆ†ï¼‰
        # è®°å¿†åº“å‡†å¤‡ï¼ˆStep-I/IIå‡éœ€ï¼Œä»æ¨¡å‹è·å–åŠ¨æ€æ›´æ–°çš„è®°å¿†åº“ï¼‰
        vis_memory = main_net.vis_memory  # å¯è§å…‰è®°å¿†åº“ï¼ˆshape=[C_vis, feat_dim]ï¼‰
        ir_memory = main_net.ir_memory    # çº¢å¤–è®°å¿†åº“ï¼ˆshape=[C_ir, feat_dim]ï¼‰
        vis_pid2idx = main_net.vis_pid2idx  # å¯è§å…‰pidâ†’è®°å¿†åº“ç´¢å¼•
        ir_pid2idx = main_net.ir_pid2idx    # çº¢å¤–pidâ†’è®°å¿†åº“ç´¢å¼•

        if stage == "step_i":
            # Step-Iï¼šä»…è®¡ç®—å•æ¨¡æ€NCEæŸå¤±
            loss_inputs = {
                "stage": "step_i",
                "vis_feats": vis_feat, "vis_labels": label_vis,
                "vis_memory": vis_memory, "vis_pid2idx": vis_pid2idx,
                "ir_feats": ir_feat, "ir_labels": label_ir,
                "ir_memory": ir_memory, "ir_pid2idx": ir_pid2idx
            }
        else:
            # Step-IIï¼šè®¡ç®—NCE + CC + IPCCæŸå¤±ï¼Œéœ€CAPå…³è”å­—å…¸ä¸å‚è€ƒè®°å¿†åº“
            if cap_mapping is None or ref_memory is None:
                raise ValueError("Step-IIè®­ç»ƒéœ€ä¼ å…¥CAPå…³è”å­—å…¸ï¼ˆcap_mappingï¼‰ä¸IPCCå‚è€ƒè®°å¿†åº“ï¼ˆref_memoryï¼‰")
            
            # å‡†å¤‡IPCCæ¨¡å—è¾“å…¥ï¼ˆä»…å¤æ‚è¯¾ç¨‹æ ·æœ¬ï¼Œæ­¤å¤„ç®€åŒ–ä¸ºæ‰€æœ‰Step-IIæ ·æœ¬ï¼‰
            # ï¼ˆå®é™…éœ€æŒ‰è¯¾ç¨‹æ©ç ç­›é€‰å¤æ‚æ ·æœ¬ï¼Œæ­¤å¤„ä¸ºç¤ºä¾‹ï¼‰
            ipcc_instance_feats = torch.cat([vis_feat, ir_feat], dim=0)  # å¤æ‚å®ä¾‹ç‰¹å¾
            # åŸå‹ç‰¹å¾ï¼šåŒæ¨¡æ€åŸå‹ï¼ˆè®°å¿†åº“ä¸­å¯¹åº”èšç±»ä¸­å¿ƒï¼‰+ è·¨æ¨¡æ€åŸå‹ï¼ˆCAPå…³è”ï¼‰
            vis_proto_feats = vis_memory[torch.tensor([vis_pid2idx[pid.item()] for pid in label_vis], device=vis_feat.device)]
            ir_proto_feats = ir_memory[torch.tensor([ir_pid2idx[pid.item()] for pid in label_ir], device=ir_feat.device)]
            ipcc_proto_feats = torch.cat([vis_proto_feats, ir_proto_feats], dim=0)

            loss_inputs = {
                "stage": "step_ii",
                # NCEæŸå¤±è¾“å…¥ï¼ˆä¸Step-Iä¸€è‡´ï¼‰
                "vis_feats": vis_feat, "vis_labels": label_vis,
                "vis_memory": vis_memory, "vis_pid2idx": vis_pid2idx,
                "ir_feats": ir_feat, "ir_labels": label_ir,
                "ir_memory": ir_memory, "ir_pid2idx": ir_pid2idx,
                # CCæŸå¤±è¾“å…¥ï¼ˆCAPå…³è”å­—å…¸ï¼‰
                "cap_vis2ir": cap_mapping["vis2ir"], "cap_ir2vis": cap_mapping["ir2vis"],
                # IPCCæŸå¤±è¾“å…¥
                "ipcc_instance_feats": ipcc_instance_feats,
                "ipcc_proto_feats": ipcc_proto_feats,
                "ref_memory": ref_memory
            }

        # 4.4 è®¡ç®—æ€»æŸå¤±ä¸åˆ†é¡¹æŒ‡æ ‡
        total_loss, loss_metrics = total_loss_fn(**loss_inputs)

        # 4.5 åå‘ä¼ æ’­ä¸ä¼˜åŒ–
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        # 4.6 æ›´æ–°æŒ‡æ ‡ä¸ç»Ÿè®¡
        # æ›´æ–°æŸå¤±æŒ‡æ ‡
        metrics["total_loss"].update(total_loss.item(), B_vis + B_ir)
        metrics["nce_loss"].update(
            loss_metrics["vis_nce_loss"] + loss_metrics["ir_nce_loss"],
            B_vis + B_ir
        )
        if stage == "step_ii":
            metrics["cc_loss"].update(loss_metrics["cc_loss"], B_vis + B_ir)
            metrics["ipcc_loss"].update(loss_metrics["ipcc_loss"], B_vis + B_ir)
            # æ›´æ–°CCå‡†ç¡®ç‡ä¸IPCCæ¦‚ç‡ç›¸ä¼¼åº¦
            acc_metrics["cc_acc_vis2ir"] += loss_metrics["vis2ir_acc"] * B_vis
            acc_metrics["cc_acc_ir2vis"] += loss_metrics["ir2vis_acc"] * B_ir
            acc_metrics["ipcc_prob_sim"] += loss_metrics["ipcc_prob_sim"] * (B_vis + B_ir)

        # æ›´æ–°NCEå‡†ç¡®ç‡
        acc_metrics["nce_acc_vis"] += loss_metrics["vis_nce_acc"] * B_vis
        acc_metrics["nce_acc_ir"] += loss_metrics["ir_nce_acc"] * B_ir

        # æ›´æ–°æ—¶é—´æŒ‡æ ‡
        metrics["batch_time"].update(time.time() - end)
        end = time.time()

        # 4.7 æ‰“å°è®­ç»ƒæ—¥å¿—
        if batch_id % print_freq == 0:
            log_msg = f"Epoch: [{epoch}/{ENGINE_CONFIG['train_stage']['total_epoch']}] " \
                      f"Stage: {stage} " \
                      f"Batch: [{batch_id}/{len(trainloader)}] " \
                      f"LR: {current_lr:.6f} " \
                      f"BatchTime: {metrics['batch_time'].val:.3f}({metrics['batch_time'].avg:.3f}) " \
                      f"TotalLoss: {metrics['total_loss'].val:.4f}({metrics['total_loss'].avg:.4f}) " \
                      f"NCELoss: {metrics['nce_loss'].val:.4f}({metrics['nce_loss'].avg:.4f}) "
            if stage == "step_ii":
                log_msg += f"CCLoss: {metrics['cc_loss'].val:.4f}({metrics['cc_loss'].avg:.4f}) " \
                           f"IPCCLoss: {metrics['ipcc_loss'].val:.4f}({metrics['ipcc_loss'].avg:.4f}) "
            # æ‰“å°å‡†ç¡®ç‡
            log_msg += f"NCELossVisAcc: {acc_metrics['nce_acc_vis']/num_samples['vis']:.4f} " \
                       f"NCELossIrAcc: {acc_metrics['nce_acc_ir']/num_samples['ir']:.4f} "
            if stage == "step_ii":
                log_msg += f"CCVis2IrAcc: {acc_metrics['cc_acc_vis2ir']/num_samples['vis']:.4f} " \
                           f"CCIr2VisAcc: {acc_metrics['cc_acc_ir2vis']/num_samples['ir']:.4f} "
            print(log_msg)
            if logger is not None:
                logger.write(log_msg + "\n")
                logger.flush()

    # 5. æ•´ç†è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ï¼ˆè®¡ç®—å¹³å‡å‡†ç¡®ç‡ï¼‰
    train_stats = {
        "epoch": epoch,
        "stage": stage,
        "lr": current_lr,
        "total_loss": metrics["total_loss"].avg,
        "nce_loss": metrics["nce_loss"].avg,
        "nce_acc_vis": acc_metrics["nce_acc_vis"] / num_samples["vis"],
        "nce_acc_ir": acc_metrics["nce_acc_ir"] / num_samples["ir"]
    }
    if stage == "step_ii":
        train_stats.update({
            "cc_loss": metrics["cc_loss"].avg,
            "ipcc_loss": metrics["ipcc_loss"].avg,
            "cc_acc_vis2ir": acc_metrics["cc_acc_vis2ir"] / num_samples["vis"],
            "cc_acc_ir2vis": acc_metrics["cc_acc_ir2vis"] / num_samples["ir"],
            "ipcc_prob_sim": acc_metrics["ipcc_prob_sim"] / (num_samples["vis"] + num_samples["ir"])
        })

    # 6. å†™å…¥TensorBoardï¼ˆè‹¥æœ‰ï¼‰
    if writer is not None:
        writer.add_scalar(f"{stage}/LR", current_lr, epoch)
        writer.add_scalar(f"{stage}/TotalLoss", train_stats["total_loss"], epoch)
        writer.add_scalar(f"{stage}/NCELoss", train_stats["nce_loss"], epoch)
        writer.add_scalar(f"{stage}/NCEAccVis", train_stats["nce_acc_vis"], epoch)
        writer.add_scalar(f"{stage}/NCEAccIr", train_stats["nce_acc_ir"], epoch)
        if stage == "step_ii":
            writer.add_scalar(f"{stage}/CCLoss", train_stats["cc_loss"], epoch)
            writer.add_scalar(f"{stage}/IPCCLoss", train_stats["ipcc_loss"], epoch)
            writer.add_scalar(f"{stage}/CCAccVis2Ir", train_stats["cc_acc_vis2ir"], epoch)
            writer.add_scalar(f"{stage}/CCAccIr2Vis", train_stats["cc_acc_ir2vis"], epoch)

    return train_stats


def tester(
    args,
    epoch: int,
    main_net: torch.nn.Module,
    test_loader: Dict[str, torch.utils.data.DataLoader],  # æµ‹è¯•åŠ è½½å™¨å­—å…¸ï¼ˆå«query/gallï¼‰
    test_info: Dict[str, np.ndarray],  # æµ‹è¯•ä¿¡æ¯ï¼ˆquery_pids/gall_pids/query_cams/gall_camsï¼‰
    dataset: str = "sysu",
    test_mode: str = "all",  # SYSUç”¨"all"/"indoor"ï¼ŒRegDBç”¨"vis2thermal"/"thermal2vis"
    feat_dim: int = ENGINE_CONFIG["feature"]["feat_dim"],
    logger: Optional[CSANetLogger] = None,
    writer: Optional[SummaryWriter] = None,
) -> Tuple[np.ndarray, float, float, Dict[str, float]]:
    """
    CSANetæµ‹è¯•å™¨ï¼šé€‚é…SYSU-MM01/RegDBçš„è®ºæ–‡æµ‹è¯•åè®®ï¼ˆğŸ”¶1-204ã€ğŸ”¶1-205ï¼‰
    Args:
        test_loader: æµ‹è¯•åŠ è½½å™¨å­—å…¸ï¼Œå«"query_loader"ï¼ˆæŸ¥è¯¢åŠ è½½å™¨ï¼‰å’Œ"gall_loader"ï¼ˆç”»å»ŠåŠ è½½å™¨ï¼‰
        test_info: æµ‹è¯•ä¿¡æ¯å­—å…¸ï¼Œå«"query_pids"/"gall_pids"/"query_cams"/"gall_cams"
        test_mode: æµ‹è¯•æ¨¡å¼ï¼ˆéœ€ä¸æ•°æ®é›†åŒ¹é…ï¼‰
    Returns:
        cmc: CMCæ›²çº¿æ•°ç»„
        mAP: å¹³å‡ç²¾åº¦
        mINP: å¹³å‡é€†åºç²¾åº¦
        test_stats: æ ¼å¼åŒ–æµ‹è¯•ç»Ÿè®¡ï¼ˆå«Rank-1/5/10/20ã€mAPã€mINPï¼‰
    """
    # 1. æ ¡éªŒè¾“å…¥åˆæ³•æ€§
    required_keys = ["query_loader", "gall_loader"]
    if not all(key in test_loader for key in required_keys):
        raise KeyError(f"test_loaderéœ€åŒ…å«{required_keys}é”®")
    required_info = ["query_pids", "gall_pids", "query_cams", "gall_cams"]
    if not all(key in test_info for key in required_info):
        raise KeyError(f"test_infoéœ€åŒ…å«{required_info}é”®")
    
    query_loader = test_loader["query_loader"]
    gall_loader = test_loader["gall_loader"]
    query_pids, gall_pids = test_info["query_pids"], test_info["gall_pids"]
    query_cams, gall_cams = test_info["query_cams"], test_info["gall_cams"]

    # æ ¡éªŒç‰¹å¾ç»´åº¦
    if feat_dim != ENGINE_CONFIG["feature"]["feat_dim"]:
        print(f"è­¦å‘Šï¼šç‰¹å¾ç»´åº¦{feat_dim}ä¸è®ºæ–‡é»˜è®¤{ENGINE_CONFIG['feature']['feat_dim']}ä¸ä¸€è‡´ï¼Œå¯èƒ½å½±å“è¯„ä¼°")

    # 2. æ¨¡å‹åˆ‡æ¢è‡³è¯„ä¼°æ¨¡å¼
    main_net.eval()
    print(f"\n=== å¼€å§‹æµ‹è¯•ï¼ˆæ•°æ®é›†ï¼š{dataset}ï¼Œæ¨¡å¼ï¼š{test_mode}ï¼ŒEpochï¼š{epoch}ï¼‰===")

    # 3. æå–ç”»å»Šç‰¹å¾
    print("Step 1/3ï¼šæå–ç”»å»Šç‰¹å¾...")
    ngall = len(gall_pids)
    gall_feat = np.zeros((ngall, feat_dim), dtype=np.float32)
    ptr = 0
    t_start = time.time()

    with torch.no_grad():
        for batch_idx, (input_gall, label_gall) in enumerate(gall_loader):
            batch_size = input_gall.size(0)
            input_gall = Variable(input_gall.cuda())
            
            # ç¡®å®šç”»å»Šæ¨¡æ€ï¼ˆSYSUï¼šç”»å»Šä¸ºå¯è§å…‰ï¼›RegDBï¼šæŒ‰æ¨¡å¼åŒºåˆ†ï¼‰
            if dataset == "sysu":
                # SYSUæµ‹è¯•ï¼šç”»å»Šå›ºå®šä¸ºå¯è§å…‰ï¼ˆmodal=1ï¼‰
                modal_gall = 1
            else:  # regdb
                # RegDBæµ‹è¯•ï¼švis2thermalâ†’ç”»å»Šä¸ºçº¢å¤–ï¼ˆmodal=2ï¼‰ï¼›thermal2visâ†’ç”»å»Šä¸ºå¯è§å…‰ï¼ˆmodal=1ï¼‰
                modal_gall = 2 if test_mode == "vis2thermal" else 1
            
            # æ¨¡å‹æå–ç‰¹å¾ï¼ˆä»…è¾“å‡ºç‰¹å¾ï¼Œä¸è¾“å‡ºå…¶ä»–å‚æ•°ï¼‰
            feat_gall = main_net(input_gall, input_gall, modal=modal_gall, train_set=False)
            # æ ¡éªŒç‰¹å¾ç»´åº¦
            if feat_gall.size(1) != feat_dim:
                raise RuntimeError(f"ç”»å»Šç‰¹å¾ç»´åº¦{feat_gall.size(1)}ä¸é¢„æœŸ{feat_dim}ä¸åŒ¹é…")
            
            # å­˜å‚¨ç‰¹å¾
            gall_feat[ptr:ptr+batch_size, :] = feat_gall.detach().cpu().numpy()
            ptr += batch_size

    gall_extract_time = time.time() - t_start
    print(f"ç”»å»Šç‰¹å¾æå–å®Œæˆï¼š{ngall}ä¸ªæ ·æœ¬ï¼Œè€—æ—¶{gall_extract_time:.3f}s")

    # 4. æå–æŸ¥è¯¢ç‰¹å¾
    print("Step 2/3ï¼šæå–æŸ¥è¯¢ç‰¹å¾...")
    nquery = len(query_pids)
    query_feat = np.zeros((nquery, feat_dim), dtype=np.float32)
    ptr = 0
    t_start = time.time()

    with torch.no_grad():
        for batch_idx, (input_query, label_query) in enumerate(query_loader):
            batch_size = input_query.size(0)
            input_query = Variable(input_query.cuda())
            
            # ç¡®å®šæŸ¥è¯¢æ¨¡æ€ï¼ˆSYSUï¼šæŸ¥è¯¢å›ºå®šä¸ºçº¢å¤–ï¼ˆmodal=2ï¼‰ï¼›RegDBï¼šæŒ‰æ¨¡å¼åŒºåˆ†ï¼‰
            if dataset == "sysu":
                modal_query = 2
            else:  # regdb
                modal_query = 1 if test_mode == "vis2thermal" else 2
            
            # æ¨¡å‹æå–ç‰¹å¾
            feat_query = main_net(input_query, input_query, modal=modal_query, train_set=False)
            if feat_query.size(1) != feat_dim:
                raise RuntimeError(f"æŸ¥è¯¢ç‰¹å¾ç»´åº¦{feat_query.size(1)}ä¸é¢„æœŸ{feat_dim}ä¸åŒ¹é…")
            
            # å­˜å‚¨ç‰¹å¾
            query_feat[ptr:ptr+batch_size, :] = feat_query.detach().cpu().numpy()
            ptr += batch_size

    query_extract_time = time.time() - t_start
    print(f"æŸ¥è¯¢ç‰¹å¾æå–å®Œæˆï¼š{nquery}ä¸ªæ ·æœ¬ï¼Œè€—æ—¶{query_extract_time:.3f}s")

    # 5. è®¡ç®—è·ç¦»çŸ©é˜µä¸è¯„ä¼°
    print("Step 3/3ï¼šè®¡ç®—è·ç¦»çŸ©é˜µä¸è¯„ä¼°...")
    t_start = time.time()

    # è®¡ç®—æ¬§å¼è·ç¦»çŸ©é˜µï¼ˆè®ºæ–‡ä¸­è·ç¦»è®¡ç®—ç»Ÿä¸€ç”¨æ¬§å¼è·ç¦»ï¼ŒğŸ”¶1-131ã€ğŸ”¶1-187ï¼‰
    distmat = compute_euclidean_distance(query_feat, gall_feat)

    # æŒ‰æ•°æ®é›†è°ƒç”¨å¯¹åº”è¯„ä¼°å‡½æ•°
    if dataset == "sysu":
        # SYSU-MM01è¯„ä¼°ï¼šéœ€ä¼ é€’ç›¸æœºIDï¼ŒåŒºåˆ†æµ‹è¯•æ¨¡å¼
        cmc, mAP, mINP = eval_sysu(
            distmat=distmat,
            q_pids=query_pids,
            g_pids=gall_pids,
            q_camids=query_cams,
            g_camids=gall_cams,
            mode=test_mode
        )
    elif dataset == "regdb":
        # RegDBè¯„ä¼°ï¼šéœ€ä¼ é€’ç›¸æœºIDï¼ˆåŠ¨æ€é€‚é…åŒå‘æ¨¡å¼ï¼‰
        cmc, mAP, mINP = eval_regdb(
            distmat=distmat,
            q_pids=query_pids,
            g_pids=gall_pids,
            q_camids=query_cams,
            g_camids=gall_cams,
            mode=test_mode
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ï¼š{dataset}ï¼Œä»…æ”¯æŒ'sysu'/'regdb'")

    eval_time = time.time() - t_start
    print(f"è¯„ä¼°å®Œæˆï¼šè€—æ—¶{eval_time:.3f}s")

    # 6. æ ¼å¼åŒ–æµ‹è¯•ç»“æœï¼ˆé€‚é…è®ºæ–‡æŠ¥å‘Šæ ¼å¼ï¼‰
    test_stats = format_metrics(
        cmc=cmc,
        mAP=mAP,
        mINP=mINP,
        dataset=dataset,
        mode=test_mode
    )

    # 7. æ‰“å°ä¸è®°å½•æµ‹è¯•æ—¥å¿—
    log_msg = f"\n=== æµ‹è¯•ç»“æœï¼ˆEpochï¼š{epoch}ï¼Œæ•°æ®é›†ï¼š{dataset}ï¼Œæ¨¡å¼ï¼š{test_mode}ï¼‰==="
    log_msg += f"\nRank-1: {test_stats['Rank-1']:.2f}% | Rank-5: {test_stats['Rank-5']:.2f}% " \
               f"| Rank-10: {test_stats['Rank-10']:.2f}% | Rank-20: {test_stats['Rank-20']:.2f}%"
    log_msg += f"\nmAP: {test_stats['mAP']:.2f}% | mINP: {test_stats['mINP']:.2f}%"
    log_msg += f"\nç‰¹å¾æå–æ€»è€—æ—¶ï¼š{gall_extract_time + query_extract_time:.3f}s | è¯„ä¼°è€—æ—¶ï¼š{eval_time:.3f}s"
    print(log_msg)
    if logger is not None:
        logger.write(log_msg + "\n")
        logger.flush()

    # 8. å†™å…¥TensorBoardï¼ˆè‹¥æœ‰ï¼‰
    if writer is not None:
        writer.add_scalar(f"test_{dataset}_{test_mode}/Rank-1", test_stats["Rank-1"], epoch)
        writer.add_scalar(f"test_{dataset}_{test_mode}/mAP", test_stats["mAP"], epoch)
        writer.add_scalar(f"test_{dataset}_{test_mode}/mINP", test_stats["mINP"], epoch)

    return cmc, mAP, mINP, test_stats


def compute_euclidean_distance(query_feat: np.ndarray, gall_feat: np.ndarray) -> np.ndarray:
    """
    è®¡ç®—æ¬§å¼è·ç¦»çŸ©é˜µï¼ˆè®ºæ–‡ä¸­æ‰€æœ‰è·ç¦»è®¡ç®—å‡ç”¨æ¬§å¼è·ç¦»ï¼ŒğŸ”¶1-131ã€ğŸ”¶1-187ï¼‰
    ä¼˜åŒ–å®ç°ï¼šåŸºäºçŸ©é˜µåˆ†è§£ï¼Œé¿å…å¹¿æ’­å¯¼è‡´çš„å†…å­˜æº¢å‡º
    """
    # å…¬å¼ï¼š||a - b||Â² = ||a||Â² + ||b||Â² - 2abáµ€ï¼Œå¼€æ ¹å·åä¸ºæ¬§å¼è·ç¦»
    eps = 1e-12  # é¿å…æ•°å€¼ä¸ç¨³å®š
    query_sq = np.sum(query_feat ** 2, axis=1, keepdims=True)  # (nquery, 1)
    gall_sq = np.sum(gall_feat ** 2, axis=1, keepdims=True)    # (ngall, 1)
    dot_product = np.matmul(query_feat, gall_feat.T)           # (nquery, ngall)
    # è®¡ç®—è·ç¦»å¹¶ç¡®ä¿éè´Ÿï¼ˆé¿å…æµ®ç‚¹è¯¯å·®å¯¼è‡´çš„è´Ÿå€¼ï¼‰
    dist_sq = query_sq + gall_sq.T - 2 * dot_product
    dist_sq = np.maximum(dist_sq, eps)  # æˆªæ–­è´Ÿå€¼
    distmat = np.sqrt(dist_sq)
    return distmat

def csanet_train_pipeline(
    args,
    main_net: torch.nn.Module,
    train_loaders: Dict[str, torch.utils.data.DataLoader],  # è®­ç»ƒåŠ è½½å™¨ï¼ˆstep_i/step_iiï¼‰
    test_loaders: Dict[str, Dict[str, torch.utils.data.DataLoader]],  # æµ‹è¯•åŠ è½½å™¨ï¼ˆæŒ‰æ•°æ®é›†-æ¨¡å¼åŒºåˆ†ï¼‰
    test_infos: Dict[str, Dict[str, np.ndarray]],  # æµ‹è¯•ä¿¡æ¯ï¼ˆæŒ‰æ•°æ®é›†-æ¨¡å¼åŒºåˆ†ï¼‰
    optimizer: torch.optim.Optimizer,
    total_loss_fn: CSANetTotalLoss,
    logger: CSANetLogger,
    writer: Optional[SummaryWriter] = None,
) -> Dict[str, List[Dict[str, float]]]:
    """
    CSANetå®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆStep-Iâ†’Step-IIï¼‰ï¼Œéµå¾ªè®ºæ–‡Algorithm 1ï¼ˆğŸ”¶1-210ï¼‰
    Returns:
        all_stats: æ‰€æœ‰è®­ç»ƒ/æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåç»­åˆ†æä¸è®ºæ–‡ç»˜å›¾ï¼‰
    """
    all_stats = {"train": [], "test": []}
    best_rank1 = {"sysu_all": 0.0, "sysu_indoor": 0.0, "regdb_vis2thermal": 0.0, "regdb_thermal2vis": 0.0}

    # åˆå§‹åŒ–Step-I/IIä¸“å±ç»„ä»¶ï¼ˆå¦‚CAPæ¨¡å—ã€IPCCæ¨¡å—ã€è®°å¿†åº“ï¼‰
    # ï¼ˆæ­¤å¤„ç®€åŒ–ï¼Œå®é™…éœ€åˆå§‹åŒ–main_netä¸­çš„vis_memory/ir_memoryã€capæ¨¡å—ã€ipccæ¨¡å—ï¼‰
    main_net.init_memory(feat_dim=ENGINE_CONFIG["feature"]["feat_dim"])  # åˆå§‹åŒ–è®°å¿†åº“
    cap_mapping = {"vis2ir": {}, "ir2vis": {}}  # CAPå…³è”å­—å…¸ï¼ˆStep-IIåŠ¨æ€æ›´æ–°ï¼‰
    ref_memory = None  # IPCCå‚è€ƒè®°å¿†åº“ï¼ˆStep-IIä»Step-Iç®€å•è¯¾ç¨‹è®°å¿†åº“è·å–ï¼‰

    # è¿­ä»£è®­ç»ƒï¼ˆæ€»epochï¼š60ï¼‰
    for epoch in range(1, ENGINE_CONFIG["train_stage"]["total_epoch"] + 1):
        stage = get_training_stage(epoch)
        print(f"\n===================== Epoch {epoch}/{ENGINE_CONFIG['train_stage']['total_epoch']}ï¼ˆ{stage}ï¼‰=====================")

        # 1. è®­ç»ƒé˜¶æ®µ
        train_loader = train_loaders[stage]
        # Step-IIéœ€å‡†å¤‡è¯¾ç¨‹æ©ç ä¸CAPå…³è”å­—å…¸ï¼ˆæ­¤å¤„ç®€åŒ–ä¸ºåŠ¨æ€è·å–ï¼‰
        if stage == "step_ii":
            # ä»æ¨¡å‹è·å–è¯¾ç¨‹æ©ç ï¼ˆTBGMæ¨¡å—è¾“å‡ºï¼‰
            curriculum_mask_vis = main_net.tbgm.curriculum_mask_vis
            curriculum_mask_ir = main_net.tbgm.curriculum_mask_ir
            # ä»CAPæ¨¡å—è·å–å…³è”å­—å…¸
            cap_mapping = main_net.cap.get_mapping()
            # ä»Step-Iè®°å¿†åº“è·å–å‚è€ƒè®°å¿†åº“ï¼ˆç®€å•è¯¾ç¨‹è®°å¿†åº“ï¼‰
            ref_memory = main_net.vis_memory  # ç®€åŒ–ä¸ºå¯è§å…‰ç®€å•è¯¾ç¨‹è®°å¿†åº“
        else:
            curriculum_mask_vis = None
            curriculum_mask_ir = None

        # è°ƒç”¨è®­ç»ƒå™¨
        train_stats = trainer(
            args=args,
            epoch=epoch,
            main_net=main_net,
            optimizer=optimizer,
            trainloader=train_loader,
            total_loss_fn=total_loss_fn,
            logger=logger,
            writer=writer,
            curriculum_mask_vis=curriculum_mask_vis,
            curriculum_mask_ir=curriculum_mask_ir,
            cap_mapping=cap_mapping,
            ref_memory=ref_memory,
            print_freq=args.print_freq
        )
        all_stats["train"].append(train_stats)

        # 2. æµ‹è¯•é˜¶æ®µï¼ˆæ¯5 epochæµ‹è¯•ä¸€æ¬¡ï¼Œæˆ–Step-IIå…¨æµ‹ï¼Œé€‚é…è®ºæ–‡å®éªŒï¼‰
        if epoch % 5 == 0 or epoch == ENGINE_CONFIG["train_stage"]["total_epoch"]:
            test_stats_epoch = {"epoch": epoch, "stage": stage, "results": {}}
            # éå†æ‰€æœ‰æ•°æ®é›†ä¸æµ‹è¯•æ¨¡å¼
            for dataset in test_loaders.keys():
                for test_mode in test_loaders[dataset].keys():
                    cmc, mAP, mINP, test_stats = tester(
                        args=args,
                        epoch=epoch,
                        main_net=main_net,
                        test_loader=test_loaders[dataset][test_mode],
                        test_info=test_infos[dataset][test_mode],
                        dataset=dataset,
                        test_mode=test_mode,
                        logger=logger,
                        writer=writer
                    )
                    test_stats_epoch["results"][f"{dataset}_{test_mode}"] = test_stats

                    # è®°å½•æœ€ä½³ç»“æœ
                    key = f"{dataset}_{test_mode}"
                    if test_stats["Rank-1"] > best_rank1[key]:
                        best_rank1[key] = test_stats["Rank-1"]
                        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæ­¤å¤„ç®€åŒ–ï¼Œå®é™…éœ€è°ƒç”¨torch.saveï¼‰
                        print(f"=== æœ€ä½³{key}æ¨¡å‹æ›´æ–°ï¼šRank-1ä»{best_rank1[key]:.2f}%æå‡è‡³{test_stats['Rank-1']:.2f}% ===")

            all_stats["test"].append(test_stats_epoch)

    # æ‰“å°æœ€ç»ˆæœ€ä½³ç»“æœ
    print("\n===================== è®­ç»ƒå®Œæˆï¼šæœ€ä½³ç»“æœæ±‡æ€» =====================")
    for key, rank1 in best_rank1.items():
        print(f"{key} æœ€ä½³Rank-1: {rank1:.2f}%")
    if logger is not None:
        logger.write("\næœ€ä½³ç»“æœæ±‡æ€»ï¼š\n")
        for key, rank1 in best_rank1.items():
            logger.write(f"{key}: Rank-1={rank1:.2f}%\n")
        logger.flush()

    return all_stats