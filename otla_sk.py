import time
import numpy as np
import torch
import torch.nn as nn
from typing import Tuple, Dict, Optional
from utils import sort_list_with_unique_index, count_curriculum_anchors  # å¤ç”¨utils.pyçš„è¯¾ç¨‹ç»Ÿè®¡å·¥å…·
from sklearn.metrics import adjusted_rand_score  # ç¡®ä¿å¤´éƒ¨å·²å¯¼å…¥è¯¥å‡½æ•°ï¼ˆè‹¥æœªå¯¼å…¥éœ€è¡¥å……ï¼‰
from collections import defaultdict

# -------------------------- CSANet OTLA-SKå…¨å±€é…ç½®ï¼ˆä¸¥æ ¼éµå¾ªè®ºæ–‡ï¼‰ --------------------------
OTLA_SK_CONFIG = {
    "sinkhorn": {
        "error_threshold": 1e-3,  # è®ºæ–‡SKç®—æ³•è¿­ä»£ç»ˆæ­¢è¯¯å·®ï¼ˆåŸ1e-1ä¿®æ­£ï¼ŒğŸ”¶1-256ï¼‰
        "max_step": 1000,          # æœ€å¤§è¿­ä»£æ­¥æ•°ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
        "lambda_sk": 2.0           # SKç®—æ³•çš„æ¸©åº¦ç³»æ•°ï¼ˆè®ºæ–‡ç»éªŒå€¼ï¼‰
    },
    "curriculum": {
        "optimize_courses": ["moderate", "intricate"]  # ä»…ä¼˜åŒ–ä¸­ç­‰/å¤æ‚è¯¾ç¨‹ä¼ªæ ‡ç­¾ï¼ˆğŸ”¶1-99ï¼‰
    },
    "confidence": {
        "min_confidence": 0.5      # ä¼ªæ ‡ç­¾æœ€å°ç½®ä¿¡åº¦ï¼ˆç­›é€‰é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼ŒğŸ”¶1-171ï¼‰
    }
}

def calculate_pseudo_confidence(
    pred_probs: np.ndarray,  # æ¨¡å‹è¾“å‡ºçš„Softmaxæ¦‚ç‡ï¼ˆshape=[N, C]ï¼‰
    pseudo_labels: np.ndarray  # ä¼ªæ ‡ç­¾ï¼ˆshape=[N]ï¼‰
) -> np.ndarray:
    """
    è®¡ç®—ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦ï¼ˆè®ºæ–‡è¦æ±‚é‡åŒ–ç½®ä¿¡åº¦ï¼ŒğŸ”¶1-171ï¼‰
    Args:
        pred_probs: æ¨¡å‹è¾“å‡ºçš„ç±»åˆ«æ¦‚ç‡ï¼ˆSoftmaxåï¼‰
        pseudo_labels: ä¼ªæ ‡ç­¾ï¼ˆæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«ï¼‰
    Returns:
        confidence: æ¯ä¸ªä¼ªæ ‡ç­¾çš„ç½®ä¿¡åº¦ï¼ˆå¯¹åº”ç±»åˆ«çš„æ¦‚ç‡å€¼ï¼Œshape=[N]ï¼‰
    """
    N = len(pseudo_labels)
    confidence = np.zeros(N)
    for i in range(N):
        confidence[i] = pred_probs[i, pseudo_labels[i]]
    return confidence

def cpu_sk_ir_trainloader(
    args, 
    main_net: nn.Module, 
    trainloader: torch.utils.data.DataLoader, 
    tIndex: np.ndarray,  # çº¢å¤–æ ·æœ¬ç´¢å¼•ï¼ˆå¯¹åº”trainloaderä¸­çš„çº¢å¤–æ ·æœ¬ï¼‰
    n_class: int,  # ç±»åˆ«æ•°ï¼ˆä¼ªæ ‡ç­¾æœ€å¤§ç±»åˆ«ï¼‰
    curriculum_mask: Optional[np.ndarray] = None,  # çº¢å¤–è¯¾ç¨‹æ©ç ï¼ˆ0=plain,1=moderate,2=intricateï¼‰
    print_freq: int = 50
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, np.ndarray, np.ndarray, Dict[str, np.ndarray]]:
    """
    åŸºäºSKç®—æ³•ä¼˜åŒ–çº¢å¤–ä¼ªæ ‡ç­¾ï¼ˆé€‚é…CSANetåˆ†è¯¾ç¨‹éœ€æ±‚ï¼ŒğŸ”¶1-131ã€ğŸ”¶1-256ï¼‰
    Args:
        curriculum_mask: çº¢å¤–è¯¾ç¨‹æ©ç ï¼ˆç”¨äºç­›é€‰éœ€ä¼˜åŒ–çš„ä¸­ç­‰/å¤æ‚è¯¾ç¨‹æ ·æœ¬ï¼‰
        å…¶ä»–å‚æ•°ä¸åŸå‡½æ•°ä¸€è‡´
    Returns:
        ir_pseudo_label_op: SKä¼˜åŒ–åçš„ä¼ªæ ‡ç­¾ï¼ˆOTLA-SKè¾“å‡ºï¼‰
        ir_pseudo_label_mp: æœ€å¤§æ¦‚ç‡ä¼ªæ ‡ç­¾ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼‰
        ir_real_label: çœŸå®æ ‡ç­¾ï¼ˆè‹¥æœ‰ï¼Œç”¨äºè¯„ä¼°ï¼›æ— åˆ™ä¸ºå ä½ç¬¦ï¼‰
        selected_tIndex: ç­›é€‰åéœ€ä¼˜åŒ–çš„çº¢å¤–æ ·æœ¬ç´¢å¼•
        pseudo_confidence: ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦ï¼ˆSKä¼˜åŒ–åï¼‰
        stats: ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚å„è¯¾ç¨‹ä¼˜åŒ–æ ·æœ¬æ•°ã€ç½®ä¿¡åº¦åˆ†å¸ƒï¼‰
    """
    main_net.train()
    device = next(main_net.parameters()).device  # è‡ªåŠ¨è·å–æ¨¡å‹è®¾å¤‡ï¼ˆCPU/GPUï¼‰
    n_ir_total = len(tIndex)
    P = np.zeros((n_ir_total, n_class))  # å­˜å‚¨æ‰€æœ‰çº¢å¤–æ ·æœ¬çš„Softmaxæ¦‚ç‡
    ir_real_label = torch.tensor([]).to(device)
    stats = defaultdict(list)

    # 1. ç¬¬ä¸€æ­¥ï¼šæå–çº¢å¤–æ ·æœ¬çš„æ¨¡å‹é¢„æµ‹æ¦‚ç‡ï¼ˆSoftmaxåï¼‰
    print("=== å¼€å§‹æå–çº¢å¤–æ ·æœ¬é¢„æµ‹æ¦‚ç‡ ===")
    with torch.no_grad():
        for batch_idx, (input_rgb, input_ir, label_rgb, label_ir) in enumerate(trainloader):
            t_start = time.time()
            # æ•°æ®è®¾å¤‡å¯¹é½
            input_ir = input_ir.to(device)
            label_ir = label_ir.to(device)

            # æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆå‡è®¾main_netè¾“å‡ºä¸ºï¼šç‰¹å¾ã€é¢„æµ‹logitsã€å…¶ä»–è¾“å‡ºï¼‰
            # é€‚é…CSANetæ¨¡å‹è¾“å‡ºï¼šmodal=2è¡¨ç¤ºçº¢å¤–æ¨¡æ€ï¼ˆéœ€ä¸æ¨¡å‹å®šä¹‰ä¸€è‡´ï¼‰
            _, p_logits, _ = main_net(input_ir, input_ir, modal=2, train_set=False)
            p_softmax = nn.Softmax(dim=1)(p_logits).cpu().numpy()  # è½¬ä¸ºCPU numpyæ•°ç»„

            # å­˜å‚¨å½“å‰batchçš„æ¦‚ç‡ï¼ˆå¤„ç†æœ€åä¸€ä¸ªbatchçš„ç»´åº¦å¯¹é½ï¼‰
            batch_start = batch_idx * args.train_batch_size * args.num_pos
            batch_end = min((batch_idx + 1) * args.train_batch_size * args.num_pos, n_ir_total)
            P[batch_start:batch_end, :] = p_softmax[:batch_end - batch_start, :]

            # ç´¯ç§¯çœŸå®æ ‡ç­¾ï¼ˆè‹¥æœ‰ï¼‰
            if ir_real_label.numel() == 0:
                ir_real_label = label_ir
            else:
                ir_real_label = torch.cat((ir_real_label, label_ir), dim=0)

            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % print_freq == 0:
                batch_time = time.time() - t_start
                print(f"æå–é¢„æµ‹æ¦‚ç‡ï¼š[{batch_idx + 1}/{len(trainloader)}]\t"
                      f"å•batchè€—æ—¶ï¼š{batch_time:.3f}s\t"
                      f"ç´¯è®¡å¤„ç†æ ·æœ¬ï¼š{batch_end}/{n_ir_total}")

    # 2. ç¬¬äºŒæ­¥ï¼šæŒ‰è¯¾ç¨‹ç­›é€‰éœ€ä¼˜åŒ–çš„æ ·æœ¬ï¼ˆä»…ä¸­ç­‰/å¤æ‚è¯¾ç¨‹ï¼ŒğŸ”¶1-99ï¼‰
    print("\n=== æŒ‰è¯¾ç¨‹ç­›é€‰éœ€ä¼˜åŒ–çš„çº¢å¤–æ ·æœ¬ ===")
    # è¯¾ç¨‹æ©ç æ ¡éªŒä¸æ˜ å°„ï¼ˆ0=plain,1=moderate,2=intricateï¼‰
    if curriculum_mask is None:
        # æ— è¯¾ç¨‹æ©ç æ—¶ï¼Œé»˜è®¤ä¼˜åŒ–æ‰€æœ‰æ ·æœ¬ï¼ˆå…¼å®¹åŸºçº¿æ¨¡å¼ï¼‰
        selected_mask = np.ones(n_ir_total, dtype=bool)
        print("è­¦å‘Šï¼šæœªè¾“å…¥è¯¾ç¨‹æ©ç ï¼Œå°†ä¼˜åŒ–æ‰€æœ‰çº¢å¤–æ ·æœ¬ï¼ˆå»ºè®®è¡¥å……è¯¾ç¨‹åˆ’åˆ†ï¼‰")
    else:
        # ä»…ä¿ç•™ä¸­ç­‰ï¼ˆ1ï¼‰å’Œå¤æ‚ï¼ˆ2ï¼‰è¯¾ç¨‹æ ·æœ¬
        course2level = {"moderate": 1, "intricate": 2}
        target_levels = [course2level[course] for course in OTLA_SK_CONFIG["curriculum"]["optimize_courses"]]
        selected_mask = np.isin(curriculum_mask, target_levels)
    
    # ç­›é€‰æ ·æœ¬ç´¢å¼•ä¸æ¦‚ç‡
    selected_idx = np.where(selected_mask)[0]
    if len(selected_idx) == 0:
        raise RuntimeError(f"æ— ç¬¦åˆæ¡ä»¶çš„æ ·æœ¬ï¼ˆéœ€ä¼˜åŒ–è¯¾ç¨‹ï¼š{OTLA_SK_CONFIG['curriculum']['optimize_courses']}ï¼‰")
    
    # åŸºäºtIndexç­›é€‰éœ€ä¼˜åŒ–çš„æ ·æœ¬ï¼ˆç¡®ä¿ä¸trainloaderç´¢å¼•å¯¹åº”ï¼‰
    selected_tIndex = tIndex[selected_idx]
    P_selected = P[selected_idx, :]  # ç­›é€‰åçš„æ¦‚ç‡çŸ©é˜µï¼ˆshape=[N_selected, C]ï¼‰
    n_ir_selected = len(selected_tIndex)
    print(f"éœ€ä¼˜åŒ–çš„æ ·æœ¬æ•°ï¼š{n_ir_selected}/{n_ir_total}ï¼ˆè¯¾ç¨‹ï¼š{OTLA_SK_CONFIG['curriculum']['optimize_courses']}ï¼‰")

    # 3. ç¬¬ä¸‰æ­¥ï¼šæŒ‰çº¢å¤–æ ·æœ¬èº«ä»½åˆ†ç»„ï¼ˆåŸºäºtIndexçš„å”¯ä¸€èº«ä»½ï¼‰
    print("\n=== æŒ‰èº«ä»½åˆ†ç»„æ ·æœ¬ ===")
    # å¤ç”¨utils.pyçš„sort_list_with_unique_indexï¼Œè·å–æ¯ä¸ªèº«ä»½çš„æ ·æœ¬ç´¢å¼•
    _, unique_last_idx, unique_num, idx_order, unique_list = sort_list_with_unique_index(selected_tIndex)
    n_ir_unique = len(idx_order)  # å”¯ä¸€èº«ä»½æ•°
    print(f"å”¯ä¸€èº«ä»½æ•°ï¼š{n_ir_unique}ï¼ˆæ¯ä¸ªèº«ä»½å¹³å‡æ ·æœ¬æ•°ï¼š{n_ir_selected / n_ir_unique:.1f}ï¼‰")

    # è®¡ç®—æ¯ä¸ªèº«ä»½çš„å¹³å‡æ¦‚ç‡ï¼ˆé™ä½å•æ ·æœ¬å™ªå£°ï¼‰
    P_avg = np.zeros((n_ir_unique, n_class))
    for i, idx in enumerate(idx_order):
        # æ¯ä¸ªèº«ä»½çš„æ‰€æœ‰æ ·æœ¬æ¦‚ç‡å¹³å‡
        P_avg[i] = P_selected[unique_list[idx]].mean(axis=0)
        # ç»Ÿè®¡å„èº«ä»½çš„æ ·æœ¬æ•°
        stats["identity_sample_count"].append(len(unique_list[idx]))

    # 4. ç¬¬å››æ­¥ï¼šSinkhorn-Knoppç®—æ³•ä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼ˆOTLAæ ¸å¿ƒé€»è¾‘ï¼‰
    print("\n=== è¿è¡ŒSinkhorn-Knoppç®—æ³• ===")
    # å¤„ç†æ•°å€¼ç¨³å®šæ€§ï¼ˆé¿å…0æ¦‚ç‡å¯¼è‡´çš„logå¼‚å¸¸ï¼‰
    eps = 1e-12
    P_avg = np.clip(P_avg, eps, 1.0 - eps)  # æˆªæ–­æ¦‚ç‡å€¼
    # è®ºæ–‡å…¬å¼ï¼šPS = (P_avg^T)^lambda_skï¼ˆæ¸©åº¦ç³»æ•°è°ƒæ•´ï¼‰
    lambda_sk = OTLA_SK_CONFIG["sinkhorn"]["lambda_sk"]
    PS = (P_avg.T) ** lambda_sk  # shape=[C, N_unique]

    # åˆå§‹åŒ–å¯¹å¶å˜é‡
    alpha = np.ones((n_class, 1)) / n_class  # ç±»åˆ«åˆ†å¸ƒï¼ˆåˆå§‹å‡åŒ€ï¼‰
    beta = np.ones((n_ir_unique, 1)) / n_ir_unique  # èº«ä»½åˆ†å¸ƒï¼ˆåˆå§‹å‡åŒ€ï¼‰
    inv_K = 1.0 / n_class  # ç±»åˆ«æ•°å€’æ•°
    inv_N = 1.0 / n_ir_unique  # å”¯ä¸€èº«ä»½æ•°å€’æ•°

    # è¿­ä»£ä¼˜åŒ–
    err = float("inf")
    step = 0
    t_sk_start = time.time()
    max_step = OTLA_SK_CONFIG["sinkhorn"]["max_step"]
    error_threshold = OTLA_SK_CONFIG["sinkhorn"]["error_threshold"]

    while err > error_threshold and step < max_step:
        # æ›´æ–°alphaï¼ˆç±»åˆ«å¯¹å¶å˜é‡ï¼‰
        alpha = inv_K / (PS @ beta + eps)  # åŠ epsé¿å…é™¤ä»¥0
        # æ›´æ–°betaï¼ˆèº«ä»½å¯¹å¶å˜é‡ï¼‰
        beta_new = inv_N / (alpha.T @ PS + eps).T
        # è®¡ç®—è¯¯å·®ï¼ˆbetaçš„ç›¸å¯¹å˜åŒ–ï¼‰
        if step % 10 == 0:
            # é¿å…betaä¸º0å¯¼è‡´çš„nan
            valid_mask = (beta > eps) & (beta_new > eps)
            if valid_mask.sum() > 0:
                err = np.nanmean(np.abs(beta[valid_mask] / beta_new[valid_mask] - 1))
            else:
                err = error_threshold  # æ— æœ‰æ•ˆbetaæ—¶æå‰ç»ˆæ­¢
            # è®°å½•è¯¯å·®å˜åŒ–
            stats["sinkhorn_error"].append(err)
        
        beta = beta_new
        step += 1

    # SKç®—æ³•ç»“æœç»Ÿè®¡
    sk_time = time.time() - t_sk_start
    print(f"Sinkhorn-Knoppä¼˜åŒ–å®Œæˆï¼š")
    print(f"  è¿­ä»£æ­¥æ•°ï¼š{step}/{max_step}")
    print(f"  æœ€ç»ˆè¯¯å·®ï¼š{err:.6f}ï¼ˆé˜ˆå€¼ï¼š{error_threshold}ï¼‰")
    print(f"  æ€»è€—æ—¶ï¼š{sk_time:.3f}s")
    stats["sinkhorn_final_error"] = err
    stats["sinkhorn_steps"] = step
    stats["sinkhorn_time"] = sk_time

    # 5. ç¬¬äº”æ­¥ï¼šç”Ÿæˆä¼˜åŒ–åçš„ä¼ªæ ‡ç­¾ä¸ç½®ä¿¡åº¦
    print("\n=== ç”Ÿæˆä¼˜åŒ–åçš„ä¼ªæ ‡ç­¾ ===")
    # è®¡ç®—ä¼˜åŒ–åçš„è”åˆæ¦‚ç‡çŸ©é˜µ
    PS_opt = PS * np.squeeze(beta)  # æŒ‰betaè°ƒæ•´
    PS_opt = PS_opt.T * np.squeeze(alpha)  # æŒ‰alphaè°ƒæ•´
    PS_opt = PS_opt.T  # æ¢å¤shape=[C, N_unique]

    # ç”Ÿæˆæ¯ä¸ªå”¯ä¸€èº«ä»½çš„ä¼ªæ ‡ç­¾ï¼ˆå–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ï¼‰
    argmaxes_unique = np.nanargmax(PS_opt, axis=0)  # shape=[N_unique]
    # æ˜ å°„åˆ°æ‰€æœ‰ç­›é€‰åçš„æ ·æœ¬ï¼ˆæ¯ä¸ªèº«ä»½çš„æ‰€æœ‰æ ·æœ¬å…±äº«åŒä¸€ä¼ªæ ‡ç­¾ï¼‰
    ir_pseudo_label_op = np.zeros(n_ir_selected, dtype=int)
    for i, idx in enumerate(idx_order):
        sample_idx = unique_list[idx]  # å½“å‰èº«ä»½çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
        ir_pseudo_label_op[sample_idx] = argmaxes_unique[i]

    # ç”Ÿæˆæœ€å¤§æ¦‚ç‡ä¼ªæ ‡ç­¾ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼Œç”¨äºå¯¹æ¯”ï¼‰
    argmaxes_mp = np.nanargmax(P_avg, axis=1)  # æ¯ä¸ªå”¯ä¸€èº«ä»½çš„æœ€å¤§æ¦‚ç‡ç±»åˆ«
    ir_pseudo_label_mp = np.zeros(n_ir_selected, dtype=int)
    for i, idx in enumerate(idx_order):
        sample_idx = unique_list[idx]
        ir_pseudo_label_mp[sample_idx] = argmaxes_mp[i]

    # è®¡ç®—ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦ï¼ˆSKä¼˜åŒ–åï¼‰
    pseudo_confidence = calculate_pseudo_confidence(P_selected, ir_pseudo_label_op)
    # ç­›é€‰é«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾ï¼ˆæŒ‰è®ºæ–‡æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼ï¼‰
    high_conf_mask = pseudo_confidence >= OTLA_SK_CONFIG["confidence"]["min_confidence"]
    stats["high_confidence_ratio"] = high_conf_mask.sum() / n_ir_selected
    print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼ˆâ‰¥{OTLA_SK_CONFIG['confidence']['min_confidence']}ï¼‰å æ¯”ï¼š{stats['high_confidence_ratio']:.2%}")

    # 6. ç¬¬å…­æ­¥ï¼šæ•´ç†è¾“å‡ºä¸ç»Ÿè®¡ä¿¡æ¯
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    ir_pseudo_label_op = torch.LongTensor(ir_pseudo_label_op)
    ir_pseudo_label_mp = torch.LongTensor(ir_pseudo_label_mp)
    # çœŸå®æ ‡ç­¾ç­›é€‰ï¼ˆä»…ä¿ç•™éœ€ä¼˜åŒ–çš„æ ·æœ¬ï¼‰
    ir_real_label = ir_real_label[selected_idx].cpu() if ir_real_label.numel() > 0 else torch.tensor([])

    # è¯¾ç¨‹ç»Ÿè®¡ï¼ˆè‹¥æœ‰è¯¾ç¨‹æ©ç ï¼‰
    if curriculum_mask is not None:
        selected_course = curriculum_mask[selected_idx]
        for level in [1, 2]:
            course_name = "moderate" if level == 1 else "intricate"
            count = (selected_course == level).sum()
            stats[f"{course_name}_sample_count"] = count
            print(f"{course_name}è¯¾ç¨‹ä¼˜åŒ–æ ·æœ¬æ•°ï¼š{count}/{n_ir_selected}")

    # ç½®ä¿¡åº¦ç»Ÿè®¡
    stats["confidence_mean"] = pseudo_confidence.mean()
    stats["confidence_std"] = pseudo_confidence.std()
    print(f"ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦ï¼šå‡å€¼={stats['confidence_mean']:.4f}ï¼Œæ ‡å‡†å·®={stats['confidence_std']:.4f}")

    return (ir_pseudo_label_op, ir_pseudo_label_mp, ir_real_label, 
            selected_tIndex, pseudo_confidence, stats)

def evaluate_pseudo_label(
    pseudo_label: np.ndarray,  # SKä¼˜åŒ–åçš„ä¼ªæ ‡ç­¾
    true_label: np.ndarray,    # çœŸå®æ ‡ç­¾ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    confidence: np.ndarray,    # ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦
    dataset: str = "sysu",     # æ•°æ®é›†åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    course: str = "step_i_ir" # è¯¾ç¨‹/é˜¶æ®µæ ‡è¯†ï¼ˆç”¨äºæ—¥å¿—ï¼‰
) -> Dict[str, float]:
    """
    è¯„ä¼°ä¼ªæ ‡ç­¾è´¨é‡ï¼ˆè®ºæ–‡è¡¨VIIIæ ¸å¿ƒæŒ‡æ ‡ï¼ŒğŸ”¶1-256ï¼‰
    è®¡ç®—ARIï¼ˆAdjusted Rand Indexï¼‰ã€é«˜ç½®ä¿¡åº¦æ ·æœ¬å æ¯”ç­‰å…³é”®æŒ‡æ ‡
    Args:
        pseudo_label: ä¼˜åŒ–åçš„ä¼ªæ ‡ç­¾æ•°ç»„ï¼ˆshape=[N]ï¼‰
        true_label: çœŸå®æ ‡ç­¾æ•°ç»„ï¼ˆshape=[N]ï¼Œéœ€ä¸ä¼ªæ ‡ç­¾é•¿åº¦ä¸€è‡´ï¼‰
        confidence: ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦æ•°ç»„ï¼ˆshape=[N]ï¼‰
        course: å½“å‰è¯¾ç¨‹/é˜¶æ®µï¼ˆå¦‚"step_i_ir"è¡¨ç¤ºStep-Içº¢å¤–ï¼‰
    Returns:
        eval_stats: è¯„ä¼°ç»Ÿè®¡å­—å…¸ï¼ˆå«ARIã€ç½®ä¿¡åº¦ç»Ÿè®¡ç­‰ï¼‰
    """
    # 1. æ ¡éªŒè¾“å…¥åˆæ³•æ€§
    if len(pseudo_label) != len(true_label) or len(pseudo_label) != len(confidence):
        raise ValueError(
            f"è¾“å…¥æ•°ç»„é•¿åº¦ä¸åŒ¹é…ï¼šä¼ªæ ‡ç­¾={len(pseudo_label)}ï¼ŒçœŸå®æ ‡ç­¾={len(true_label)}ï¼Œç½®ä¿¡åº¦={len(confidence)}"
        )
    if len(pseudo_label) == 0:
        raise ValueError("è¾“å…¥æ•°ç»„ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°ä¼ªæ ‡ç­¾è´¨é‡")

    # 2. è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡ï¼šARIï¼ˆAdjusted Rand Indexï¼‰
    # ARIèŒƒå›´[-1,1]ï¼Œ1è¡¨ç¤ºå®Œå…¨ä¸€è‡´ï¼Œ0è¡¨ç¤ºéšæœºèšç±»ï¼ˆè®ºæ–‡è¡¨VIIIå…³é”®æŒ‡æ ‡ï¼‰
    ari = adjusted_rand_score(true_label, pseudo_label)

    # 3. ç½®ä¿¡åº¦ç»Ÿè®¡ï¼ˆé«˜ç½®ä¿¡åº¦æ ·æœ¬å æ¯”ã€å‡å€¼ã€æ ‡å‡†å·®ï¼‰
    min_confidence = OTLA_SK_CONFIG["confidence"]["min_confidence"]  # ä»å…¨å±€é…ç½®è·å–ç½®ä¿¡åº¦é˜ˆå€¼
    high_conf_mask = confidence >= min_confidence
    high_conf_ratio = high_conf_mask.sum() / len(confidence)  # é«˜ç½®ä¿¡åº¦æ ·æœ¬å æ¯”
    conf_mean = confidence.mean()  # ç½®ä¿¡åº¦å‡å€¼
    conf_std = confidence.std()    # ç½®ä¿¡åº¦æ ‡å‡†å·®

    # 4. é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„ARIï¼ˆé¢å¤–è¯„ä¼°é«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾è´¨é‡ï¼‰
    high_conf_ari = 0.0
    if high_conf_mask.sum() > 0:
        high_conf_ari = adjusted_rand_score(
            true_label[high_conf_mask],
            pseudo_label[high_conf_mask]
        )

    # 5. æ•´ç†è¯„ä¼°ç»“æœ
    eval_stats = defaultdict(float)
    eval_stats["ARI"] = ari
    eval_stats["high_confidence_ratio"] = high_conf_ratio
    eval_stats["confidence_mean"] = conf_mean
    eval_stats["confidence_std"] = conf_std
    eval_stats["high_conf_ARI"] = high_conf_ari

    # 6. æ‰“å°è¯„ä¼°æ—¥å¿—ï¼ˆé€‚é…è®ºæ–‡æ ¼å¼ï¼‰
    print(f"\n=== {dataset} {course} ä¼ªæ ‡ç­¾è´¨é‡è¯„ä¼° ===")
    print(f"ARIåˆ†æ•°ï¼ˆå…¨å±€ï¼‰: {ari:.4f}ï¼ˆ1=å®Œå…¨ä¸€è‡´ï¼Œ0=éšæœºï¼‰")
    print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼ˆâ‰¥{min_confidence}ï¼‰:")
    print(f"  å æ¯”: {high_conf_ratio:.2%}")
    print(f"  ARIåˆ†æ•°: {high_conf_ari:.4f}")
    print(f"ç½®ä¿¡åº¦ç»Ÿè®¡: å‡å€¼={conf_mean:.4f}ï¼Œæ ‡å‡†å·®={conf_std:.4f}")
    print("="*60)

    return eval_stats