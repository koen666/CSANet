import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional

# -------------------------- CSANetæŸå¤±å‡½æ•°å…¨å±€é…ç½®ï¼ˆä¸¥æ ¼éµå¾ªè®ºæ–‡ï¼‰ --------------------------
LOSS_CONFIG = {
    "temperature": 0.05,        # æ‰€æœ‰å¯¹æ¯”æŸå¤±çš„æ¸©åº¦ç³»æ•°ï¼ˆğŸ”¶1-133ã€ğŸ”¶1-182ï¼‰
    "loss_weights": {
        "lambda_nce": 1.0,      # Step-Iå•æ¨¡æ€å¯¹æ¯”æŸå¤±æƒé‡ï¼ˆğŸ”¶1-133ï¼‰
        "lambda_cc": 1.0,       # Step-IIè·¨æ¨¡æ€å¯¹æ¯”æŸå¤±æƒé‡ï¼ˆğŸ”¶1-183ï¼‰
        "lambda_ipcc": 0.5      # IPCCä¸€è‡´æ€§æŸå¤±æƒé‡ï¼ˆğŸ”¶1-197ï¼‰
    },
    "triplet": {
        "margin": 0.3           # åŸºçº¿TripletLossçš„marginï¼ˆä»…ç”¨äºå¯¹æ¯”å®éªŒï¼‰
    }
}

def normalize(x: torch.Tensor, axis: int = -1) -> torch.Tensor:
    """
    ç‰¹å¾å½’ä¸€åŒ–ï¼ˆè®ºæ–‡ä¸­æ‰€æœ‰ç›¸ä¼¼åº¦è®¡ç®—å‰å‡éœ€å½’ä¸€åŒ–ï¼ŒğŸ”¶1-133ã€ğŸ”¶1-187ï¼‰
    Args:
        x: è¾“å…¥ç‰¹å¾ï¼ˆshape=[B, d]æˆ–[C, d]ï¼ŒB=batch sizeï¼ŒC=èšç±»æ•°ï¼‰
        axis: å½’ä¸€åŒ–ç»´åº¦
    Returns:
        å•ä½é•¿åº¦å½’ä¸€åŒ–åçš„ç‰¹å¾
    """
    x = x / (torch.norm(x, p=2, dim=axis, keepdim=True).expand_as(x) + 1e-12)
    return x

class TripletLoss(nn.Module):
    """
    åŸºçº¿TripletæŸå¤±ï¼ˆä»…ç”¨äºå¯¹æ¯”å®éªŒï¼Œè®ºæ–‡ä¸­Step-I/IIä¸ä½¿ç”¨ï¼‰
    å‚è€ƒï¼šHermans et al. In Defense of the Triplet Loss for Person Re-Identification.
    é€‚é…è®ºæ–‡ä¸­margin=0.3çš„è®¾å®šï¼ˆğŸ”¶1-212 å¯¹æ¯”å®éªŒï¼‰
    """
    def __init__(self, margin: float = LOSS_CONFIG["triplet"]["margin"]):
        super(TripletLoss, self).__init__()
        self.margin = margin
        self.ranking_loss = nn.MarginRankingLoss(margin=margin)

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> Tuple[torch.Tensor, int]:
        n = inputs.size(0)
        # è®¡ç®— pairwise æ¬§æ°è·ç¦»
        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)
        dist = dist + dist.t() - 2 * torch.mm(inputs, inputs.t())
        dist = dist.clamp(min=1e-12).sqrt()  # æ•°å€¼ç¨³å®šæ€§å¤„ç†

        # æŒ–æ˜ hardest positive/negative
        mask = targets.expand(n, n).eq(targets.expand(n, n).t())
        dist_ap, dist_an = [], []
        for i in range(n):
            # Hardest positive: åŒä¸€èº«ä»½ä¸­è·ç¦»æœ€å¤§çš„æ ·æœ¬
            ap = dist[i][mask[i]].max().unsqueeze(0)
            # Hardest negative: ä¸åŒèº«ä»½ä¸­è·ç¦»æœ€å°çš„æ ·æœ¬
            an = dist[i][~mask[i]].min().unsqueeze(0)
            dist_ap.append(ap)
            dist_an.append(an)
        dist_ap = torch.cat(dist_ap)
        dist_an = torch.cat(dist_an)

        # è®¡ç®— ranking loss
        y = torch.ones_like(dist_an)
        loss = self.ranking_loss(dist_an, dist_ap, y)
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆdist_an >= dist_ap ä¸ºæ­£ç¡®ï¼‰
        correct = torch.ge(dist_an, dist_ap).sum().item()

        return loss, correct


class PredictionAlignmentLoss(nn.Module):
    """
    åºŸå¼ƒï¼šè¯¥æŸå¤±ä¸CSANetè®ºæ–‡é€»è¾‘ä¸ç¬¦ï¼Œä»…ä¿ç•™ä»£ç ç”¨äºå†å²å¯¹æ¯”
    è®ºæ–‡ä¸­ä½¿ç”¨CrossModalityContrastiveLossæ›¿ä»£è¯¥æŸå¤±å®ç°è·¨æ¨¡æ€å¯¹æ¯”
    """
    def __init__(self, lambda_vr=0.1, lambda_rv=0.5):
        super(PredictionAlignmentLoss, self).__init__()
        print("è­¦å‘Šï¼šPredictionAlignmentLosså·²åºŸå¼ƒï¼ŒCSANetæ¨èä½¿ç”¨CrossModalityContrastiveLoss")
        self.lambda_vr = lambda_vr
        self.lambda_rv = lambda_rv

    def forward(self, x_rgb, x_ir):
        # åŸé€»è¾‘ä¿ç•™ï¼Œä¸åšä¿®æ”¹
        sim_rgbtoir = torch.mm(normalize(x_rgb), normalize(x_ir).t())
        sim_irtorgb = torch.mm(normalize(x_ir), normalize(x_rgb).t())
        sim_irtoir = torch.mm(normalize(x_ir), normalize(x_ir).t())

        sim_rgbtoir = F.softmax(sim_rgbtoir, dim=1)
        sim_irtorgb = F.softmax(sim_irtorgb, dim=1)
        sim_irtoir = F.softmax(sim_irtoir, dim=1)

        KL_criterion = nn.KLDivLoss(reduction="batchmean")

        x_rgbtoir = torch.mm(sim_rgbtoir, x_ir)
        x_irtorgb = torch.mm(sim_irtorgb, x_rgb)
        x_irtoir = torch.mm(sim_irtoir, x_ir)

        x_rgb_s = F.softmax(x_rgb, dim=1)
        x_rgbtoir_ls = F.log_softmax(x_rgbtoir, dim=1)
        x_irtorgb_s = F.softmax(x_irtorgb, dim=1)
        x_irtoir_ls = F.log_softmax(x_irtoir, dim=1)

        loss_rgbtoir = KL_criterion(x_rgbtoir_ls, x_rgb_s)
        loss_irtorgb = KL_criterion(x_irtoir_ls, x_irtorgb_s)

        loss = self.lambda_vr * loss_rgbtoir + self.lambda_rv * loss_irtorgb
        return loss, sim_rgbtoir, sim_irtorgb


class ClusterNCELoss(nn.Module):
    """
    Step-Iå•æ¨¡æ€å¯¹æ¯”æŸå¤±ï¼ˆClusterNCEï¼‰ï¼šåŸºäºè®°å¿†åº“çš„èšç±»çº§å¯¹æ¯”æŸå¤±ï¼ˆğŸ”¶1-133 å…¬å¼5ï¼‰
    è¦æ±‚â€œåŒä¸€ä¼ªèº«ä»½æ ·æœ¬ç‰¹å¾ä¸è®°å¿†åº“ä¸­å¯¹åº”èšç±»ä¸­å¿ƒè·ç¦»æ›´è¿‘ï¼Œä¸å…¶ä»–èšç±»ä¸­å¿ƒè·ç¦»æ›´è¿œâ€
    """
    def __init__(self, temperature: float = LOSS_CONFIG["temperature"]):
        super(ClusterNCELoss, self).__init__()
        self.tau = temperature  # æ¸©åº¦ç³»æ•°

    def forward(
        self,
        instance_feats: torch.Tensor,  # å•æ¨¡æ€æ ·æœ¬ç‰¹å¾ï¼ˆshape=[B, d]ï¼ŒB=batch sizeï¼‰
        instance_labels: torch.Tensor, # æ ·æœ¬ä¼ªæ ‡ç­¾ï¼ˆshape=[B]ï¼‰
        memory: torch.Tensor,          # å•æ¨¡æ€è®°å¿†åº“ï¼ˆshape=[C, d]ï¼ŒC=èšç±»æ•°ï¼Œå­˜å‚¨èšç±»ä¸­å¿ƒï¼‰
        memory_pid2idx: dict           # ä¼ªèº«ä»½åˆ°è®°å¿†åº“ç´¢å¼•çš„æ˜ å°„ï¼ˆ{pid: idx}ï¼‰
    ) -> Tuple[torch.Tensor, float]:
        """
        Args:
            memory: ç”±build_modal_memoryå‡½æ•°ç”Ÿæˆï¼ˆğŸ”¶1-132 å…¬å¼3ï¼‰
            memory_pid2idx: ç¡®ä¿æ ·æœ¬ä¼ªæ ‡ç­¾èƒ½æ˜ å°„åˆ°è®°å¿†åº“å¯¹åº”è¡Œ
        Returns:
            nce_loss: å•æ¨¡æ€å¯¹æ¯”æŸå¤±å€¼
            acc: å¯¹æ¯”å‡†ç¡®ç‡ï¼ˆæ­£æ ·æœ¬ç›¸ä¼¼åº¦>è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦çš„æ¯”ä¾‹ï¼‰
        """
        # 1. ç‰¹å¾å½’ä¸€åŒ–ï¼ˆè®ºæ–‡è¦æ±‚ï¼‰
        instance_feats = normalize(instance_feats)
        memory = normalize(memory)

        # 2. è·å–æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ­£æ ·æœ¬ï¼ˆè®°å¿†åº“ä¸­è¯¥ä¼ªèº«ä»½çš„èšç±»ä¸­å¿ƒï¼‰
        batch_size = instance_feats.size(0)
        pos_memory_idx = torch.tensor([memory_pid2idx[pid.item()] for pid in instance_labels], 
                                     device=instance_feats.device)
        pos_feats = memory[pos_memory_idx]  # shape=[B, d]

        # 3. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ ·æœ¬ä¸æ‰€æœ‰è®°å¿†åº“èšç±»ä¸­å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        sim_matrix = torch.mm(instance_feats, memory.t())  # shape=[B, C]
        sim_pos = torch.diag(torch.mm(instance_feats, pos_feats.t()))  # æ­£æ ·æœ¬ç›¸ä¼¼åº¦ï¼ˆshape=[B]ï¼‰
        sim_pos = sim_pos.unsqueeze(1).expand(batch_size, sim_matrix.size(1))  # shape=[B, C]

        # 4. è®¡ç®—ClusterNCEæŸå¤±ï¼ˆå…¬å¼5ï¼‰
        logits = (sim_matrix - sim_pos) / self.tau  # çªå‡ºæ­£æ ·æœ¬ä¸è´Ÿæ ·æœ¬çš„å·®å¼‚
        labels = pos_memory_idx  # æ ‡ç­¾ä¸ºæ­£æ ·æœ¬åœ¨è®°å¿†åº“ä¸­çš„ç´¢å¼•
        nce_loss = F.cross_entropy(logits, labels)

        # 5. è®¡ç®—å¯¹æ¯”å‡†ç¡®ç‡ï¼ˆæ­£æ ·æœ¬ç›¸ä¼¼åº¦>æ‰€æœ‰è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦çš„æ ·æœ¬æ•°/æ€»æ ·æœ¬æ•°ï¼‰
        max_neg_sim, _ = torch.max(sim_matrix - sim_pos, dim=1)  # æœ€å¤§è´Ÿæ ·æœ¬ç›¸å¯¹ç›¸ä¼¼åº¦ï¼ˆåº”<0ï¼‰
        acc = (max_neg_sim < 0).sum().item() / batch_size

        return nce_loss, acc


class CrossModalityContrastiveLoss(nn.Module):
    """
    Step-IIè·¨æ¨¡æ€å¯¹æ¯”æŸå¤±ï¼ˆ\(\mathcal{L}_{CC}\)ï¼‰ï¼šåŸºäºCAPå…³è”å­—å…¸çš„åŒæ¨¡æ€å¯¹æ¯”æŸå¤±ï¼ˆğŸ”¶1-183 å…¬å¼12ï¼‰
    æ”¯æŒâ€œå¯è§å…‰â†’çº¢å¤–â€ä¸â€œçº¢å¤–â†’å¯è§å…‰â€ä¸¤ä¸ªæ–¹å‘çš„å¯¹æ¯”çº¦æŸ
    """
    def __init__(self, temperature: float = LOSS_CONFIG["temperature"]):
        super(CrossModalityContrastiveLoss, self).__init__()
        self.tau = temperature

    def _single_direction_loss(
        self,
        src_feats: torch.Tensor,    # æºæ¨¡æ€ç‰¹å¾ï¼ˆå¦‚å¯è§å…‰ï¼Œshape=[B, d]ï¼‰
        src_labels: torch.Tensor,   # æºæ¨¡æ€ä¼ªæ ‡ç­¾ï¼ˆshape=[B]ï¼‰
        tgt_memory: torch.Tensor,   # ç›®æ ‡æ¨¡æ€è®°å¿†åº“ï¼ˆå¦‚çº¢å¤–ï¼Œshape=[C_tgt, d]ï¼‰
        cap_mapping: dict           # CAPå…³è”å­—å…¸ï¼ˆsrc_pid â†’ tgt_pidï¼‰ï¼ˆğŸ”¶1-174ï¼‰
    ) -> Tuple[torch.Tensor, float]:
        """
        å•æ–¹å‘è·¨æ¨¡æ€å¯¹æ¯”æŸå¤±ï¼ˆå¦‚å¯è§å…‰â†’çº¢å¤–ï¼‰
        """
        # 1. ç‰¹å¾å½’ä¸€åŒ–
        src_feats = normalize(src_feats)
        tgt_memory = normalize(tgt_memory)

        # 2. æ˜ å°„æºæ¨¡æ€ä¼ªæ ‡ç­¾åˆ°ç›®æ ‡æ¨¡æ€ä¼ªæ ‡ç­¾ï¼ˆé€šè¿‡CAPå…³è”å­—å…¸ï¼‰
        tgt_labels = torch.tensor([cap_mapping[pid.item()] for pid in src_labels], 
                                 device=src_feats.device)
        # æ˜ å°„ç›®æ ‡æ¨¡æ€ä¼ªæ ‡ç­¾åˆ°è®°å¿†åº“ç´¢å¼•ï¼ˆå‡è®¾tgt_memoryæŒ‰pidæ’åºï¼Œç´¢å¼•=pidï¼‰
        tgt_memory_idx = tgt_labels  # è‹¥è®°å¿†åº“ç´¢å¼•ä¸pidä¸ä¸€è‡´ï¼Œéœ€ä¼ å…¥tgt_pid2idxæ˜ å°„

        # 3. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæºç‰¹å¾ä¸ç›®æ ‡è®°å¿†åº“çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        sim_matrix = torch.mm(src_feats, tgt_memory.t())  # shape=[B, C_tgt]
        sim_pos = torch.gather(sim_matrix, dim=1, index=tgt_memory_idx.unsqueeze(1))  # æ­£æ ·æœ¬ç›¸ä¼¼åº¦ï¼ˆshape=[B,1]ï¼‰

        # 4. è®¡ç®—å•æ–¹å‘æŸå¤±
        logits = sim_matrix / self.tau
        loss = F.cross_entropy(logits, tgt_memory_idx)

        # 5. è®¡ç®—å•æ–¹å‘å‡†ç¡®ç‡
        acc = (torch.argmax(logits, dim=1) == tgt_memory_idx).sum().item() / src_feats.size(0)
        return loss, acc

    def forward(
        self,
        vis_feats: torch.Tensor,    # å¯è§å…‰ç‰¹å¾ï¼ˆshape=[B, d]ï¼‰
        vis_labels: torch.Tensor,   # å¯è§å…‰ä¼ªæ ‡ç­¾ï¼ˆshape=[B]ï¼‰
        ir_feats: torch.Tensor,     # çº¢å¤–ç‰¹å¾ï¼ˆshape=[B, d]ï¼‰
        ir_labels: torch.Tensor,    # çº¢å¤–ä¼ªæ ‡ç­¾ï¼ˆshape=[B]ï¼‰
        vis_memory: torch.Tensor,   # å¯è§å…‰è®°å¿†åº“ï¼ˆshape=[C_vis, d]ï¼‰
        ir_memory: torch.Tensor,    # çº¢å¤–è®°å¿†åº“ï¼ˆshape=[C_ir, d]ï¼‰
        cap_vis2ir: dict,           # CAPå…³è”å­—å…¸ï¼ˆvis_pid â†’ ir_pidï¼‰
        cap_ir2vis: dict            # CAPå…³è”å­—å…¸ï¼ˆir_pid â†’ vis_pidï¼‰
    ) -> Tuple[torch.Tensor, float, float]:
        """
        åŒå‘è·¨æ¨¡æ€å¯¹æ¯”æŸå¤±ï¼ˆå¯è§å…‰â†’çº¢å¤– + çº¢å¤–â†’å¯è§å…‰ï¼‰
        Returns:
            cc_loss: æ€»è·¨æ¨¡æ€å¯¹æ¯”æŸå¤±
            vis2ir_acc: å¯è§å…‰â†’çº¢å¤–æ–¹å‘å‡†ç¡®ç‡
            ir2vis_acc: çº¢å¤–â†’å¯è§å…‰æ–¹å‘å‡†ç¡®ç‡
        """
        # 1. è®¡ç®—å¯è§å…‰â†’çº¢å¤–æ–¹å‘æŸå¤±
        vis2ir_loss, vis2ir_acc = self._single_direction_loss(
            src_feats=vis_feats,
            src_labels=vis_labels,
            tgt_memory=ir_memory,
            cap_mapping=cap_vis2ir
        )

        # 2. è®¡ç®—çº¢å¤–â†’å¯è§å…‰æ–¹å‘æŸå¤±
        ir2vis_loss, ir2vis_acc = self._single_direction_loss(
            src_feats=ir_feats,
            src_labels=ir_labels,
            tgt_memory=vis_memory,
            cap_mapping=cap_ir2vis
        )

        # 3. æ€»æŸå¤±ï¼ˆåŒå‘æŸå¤±ç›¸åŠ ï¼Œè®ºæ–‡å…¬å¼12ï¼‰
        cc_loss = vis2ir_loss + ir2vis_loss
        return cc_loss, vis2ir_acc, ir2vis_acc


class IPCCConsistencyLoss(nn.Module):
    """
    IPCCæ¨¡å—ä¸€è‡´æ€§æŸå¤±ï¼ˆ\(\mathcal{L}_{IPCC}\)ï¼‰ï¼šåŸºäºKLæ•£åº¦çš„å®ä¾‹-åŸå‹æ¦‚ç‡å“åº”çº¦æŸï¼ˆğŸ”¶1-194 å…¬å¼19ï¼‰
    """
    def __init__(
        self,
        temperature: float = LOSS_CONFIG["temperature"],
        weight: float = LOSS_CONFIG["loss_weights"]["lambda_ipcc"]
    ):
        super(IPCCConsistencyLoss, self).__init__()
        self.tau = temperature
        self.weight = weight
        self.kl_criterion = nn.KLDivLoss(reduction="batchmean")  # KLæ•£åº¦æŸå¤±

    def compute_prob_response(
        self,
        feats: torch.Tensor,    # å®ä¾‹/åŸå‹ç‰¹å¾ï¼ˆshape=[B, d]æˆ–[C, d]ï¼‰
        ref_memory: torch.Tensor # å‚è€ƒè®°å¿†åº“ï¼ˆç®€å•è¯¾ç¨‹è®°å¿†åº“ï¼Œshape=[C_ref, d]ï¼‰ï¼ˆğŸ”¶1-187ï¼‰
    ) -> torch.Tensor:
        """
        è®¡ç®—æ¦‚ç‡å“åº”ï¼ˆè®ºæ–‡å…¬å¼14ã€16ï¼ŒğŸ”¶1-187ï¼‰
        """
        feats = normalize(feats)
        ref_memory = normalize(ref_memory)
        sim = torch.mm(feats, ref_memory.t())  # ä½™å¼¦ç›¸ä¼¼åº¦
        prob = F.softmax(sim / self.tau, dim=1)  # æ¦‚ç‡å“åº”
        return prob

    def forward(
        self,
        instance_feats: torch.Tensor,  # å¤æ‚æ ·æœ¬å®ä¾‹ç‰¹å¾ï¼ˆshape=[B, d]ï¼‰
        prototype_feats: torch.Tensor, # å¯¹åº”åŸå‹ç‰¹å¾ï¼ˆshape=[B, d]ï¼‰
        ref_memory: torch.Tensor       # å‚è€ƒè®°å¿†åº“ï¼ˆç®€å•è¯¾ç¨‹è®°å¿†åº“ï¼ŒğŸ”¶1-187ï¼‰
    ) -> Tuple[torch.Tensor, float]:
        """
        Args:
            prototype_feats: åŒæ¨¡æ€åŸå‹+è·¨æ¨¡æ€åŸå‹çš„æ‹¼æ¥ï¼ˆæˆ–åˆ†åˆ«è®¡ç®—åæ±‚å’Œï¼‰
        Returns:
            ipcc_loss: IPCCä¸€è‡´æ€§æŸå¤±ï¼ˆåŠ æƒåï¼‰
            prob_sim: å®ä¾‹ä¸åŸå‹æ¦‚ç‡å“åº”çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè¯„ä¼°ä¸€è‡´æ€§ï¼‰
        """
        # 1. è®¡ç®—å®ä¾‹ä¸åŸå‹çš„æ¦‚ç‡å“åº”
        instance_prob = self.compute_prob_response(instance_feats, ref_memory)  # shape=[B, C_ref]
        prototype_prob = self.compute_prob_response(prototype_feats, ref_memory)  # shape=[B, C_ref]

        # 2. è®¡ç®—KLæ•£åº¦æŸå¤±ï¼ˆå…¬å¼19ï¼‰
        # åŠ epsé¿å…log(0)
        eps = 1e-10
        kl_loss = self.kl_criterion(instance_prob.log() + eps, prototype_prob + eps)

        # 3. åŠ æƒæŸå¤±ï¼ˆè®ºæ–‡å…¬å¼21ï¼‰
        ipcc_loss = self.weight * kl_loss

        # 4. è®¡ç®—æ¦‚ç‡å“åº”ç›¸ä¼¼åº¦ï¼ˆè¯„ä¼°ä¸€è‡´æ€§ï¼‰
        prob_sim = F.cosine_similarity(instance_prob, prototype_prob, dim=1).mean().item()

        return ipcc_loss, prob_sim


class CSANetTotalLoss(nn.Module):
    """
    CSANetæ€»æŸå¤±è®¡ç®—å™¨ï¼šæ•´åˆStep-I/IIæ‰€æœ‰æŸå¤±ï¼ˆå…¬å¼21ï¼ŒğŸ”¶1-197ï¼‰
    æ”¯æŒåˆ†é˜¶æ®µåˆ‡æ¢æŸå¤±ç»„åˆï¼ˆStep-I: ä»…NCEï¼›Step-II: NCE + CC + IPCCï¼‰
    """
    def __init__(self):
        super(CSANetTotalLoss, self).__init__()
        # åˆå§‹åŒ–å„æŸå¤±å‡½æ•°
        self.nce_loss = ClusterNCELoss()
        self.cc_loss = CrossModalityContrastiveLoss()
        self.ipcc_loss = IPCCConsistencyLoss()
        # æŸå¤±æƒé‡ï¼ˆè®ºæ–‡å…¬å¼21ï¼‰
        self.w_nce = LOSS_CONFIG["loss_weights"]["lambda_nce"]
        self.w_cc = LOSS_CONFIG["loss_weights"]["lambda_cc"]

    def forward(
        self,
        stage: str,  # è®­ç»ƒé˜¶æ®µï¼ˆ"step_i"æˆ–"step_ii"ï¼‰
        # Step-I/IIå…±ç”¨å‚æ•°
        vis_feats: torch.Tensor, vis_labels: torch.Tensor, vis_memory: torch.Tensor, vis_pid2idx: dict,
        ir_feats: torch.Tensor, ir_labels: torch.Tensor, ir_memory: torch.Tensor, ir_pid2idx: dict,
        # Step-IIä¸“å±å‚æ•°
        cap_vis2ir: Optional[dict] = None, cap_ir2vis: Optional[dict] = None,
        # IPCCä¸“å±å‚æ•°
        ipcc_instance_feats: Optional[torch.Tensor] = None, ipcc_proto_feats: Optional[torch.Tensor] = None,
        ref_memory: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, dict]:
        """
        Returns:
            total_loss: æ€»æŸå¤±
            loss_metrics: å„æŸå¤±åˆ†é¡¹ä¸å‡†ç¡®ç‡ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        """
        loss_metrics = {}

        # 1. è®¡ç®—Step-Iå•æ¨¡æ€å¯¹æ¯”æŸå¤±ï¼ˆå¯è§å…‰+çº¢å¤–ï¼‰
        vis_nce_loss, vis_nce_acc = self.nce_loss(
            instance_feats=vis_feats, instance_labels=vis_labels,
            memory=vis_memory, memory_pid2idx=vis_pid2idx
        )
        ir_nce_loss, ir_nce_acc = self.nce_loss(
            instance_feats=ir_feats, instance_labels=ir_labels,
            memory=ir_memory, memory_pid2idx=ir_pid2idx
        )
        total_nce_loss = self.w_nce * (vis_nce_loss + ir_nce_loss)
        loss_metrics.update({
            "vis_nce_loss": vis_nce_loss.item(), "ir_nce_loss": ir_nce_loss.item(),
            "vis_nce_acc": vis_nce_acc, "ir_nce_acc": ir_nce_acc
        })

        if stage == "step_i":
            # Step-Iï¼šä»…å•æ¨¡æ€å¯¹æ¯”æŸå¤±
            total_loss = total_nce_loss
            loss_metrics["total_loss"] = total_loss.item()
            return total_loss, loss_metrics
        elif stage == "step_ii":
            # Step-IIï¼šæ•´åˆNCE + CC + IPCCæŸå¤±
            # 2. è®¡ç®—è·¨æ¨¡æ€å¯¹æ¯”æŸå¤±
            if cap_vis2ir is None or cap_ir2vis is None:
                raise ValueError("Step-IIéœ€ä¼ å…¥CAPå…³è”å­—å…¸ï¼ˆcap_vis2irã€cap_ir2visï¼‰")
            cc_loss, vis2ir_acc, ir2vis_acc = self.cc_loss(
                vis_feats=vis_feats, vis_labels=vis_labels,
                ir_feats=ir_feats, ir_labels=ir_labels,
                vis_memory=vis_memory, ir_memory=ir_memory,
                cap_vis2ir=cap_vis2ir, cap_ir2vis=cap_ir2vis
            )
            total_cc_loss = self.w_cc * cc_loss
            loss_metrics.update({
                "cc_loss": cc_loss.item(), "vis2ir_acc": vis2ir_acc, "ir2vis_acc": ir2vis_acc
            })

            # 3. è®¡ç®—IPCCä¸€è‡´æ€§æŸå¤±ï¼ˆä»…å¤æ‚æ ·æœ¬ï¼‰
            if ipcc_instance_feats is not None and ipcc_proto_feats is not None and ref_memory is not None:
                ipcc_loss, prob_sim = self.ipcc_loss(
                    instance_feats=ipcc_instance_feats,
                    prototype_feats=ipcc_proto_feats,
                    ref_memory=ref_memory
                )
                loss_metrics.update({"ipcc_loss": ipcc_loss.item(), "ipcc_prob_sim": prob_sim})
            else:
                ipcc_loss = torch.tensor(0.0, device=vis_feats.device)
                loss_metrics["ipcc_loss"] = 0.0
                loss_metrics["ipcc_prob_sim"] = 0.0

            # 4. æ€»æŸå¤±ï¼ˆå…¬å¼21ï¼‰
            total_loss = total_nce_loss + total_cc_loss + ipcc_loss
            loss_metrics["total_loss"] = total_loss.item()
            return total_loss, loss_metrics
        else:
            raise ValueError(f"æ— æ•ˆè®­ç»ƒé˜¶æ®µï¼š{stage}ï¼Œä»…æ”¯æŒ'step_i'æˆ–'step_ii'")